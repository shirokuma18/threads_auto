#!/usr/bin/env python3
"""
CSV â†’ SQLite ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¢å­˜ã®CSVã¨posted_log.jsonã‚’SQLiteã«ç§»è¡Œ
"""

import sqlite3
import csv
import json
import argparse
import os
from datetime import datetime


DB_FILE = 'threads.db'
CSV_FILE = 'posts_schedule.csv'
LOG_FILE = 'posted_log.json'


def create_database():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ"""
    print("\n" + "="*70)
    print("ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆä¸­...")
    print("="*70)

    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    # postsãƒ†ãƒ¼ãƒ–ãƒ«
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS posts (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            csv_id TEXT UNIQUE,
            scheduled_at DATETIME NOT NULL,
            text TEXT NOT NULL,
            thread_text TEXT,
            status TEXT DEFAULT 'pending',
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,

            category TEXT,
            char_count INTEGER,
            has_emoji BOOLEAN DEFAULT 0,

            threads_post_id TEXT,
            posted_at DATETIME,
            error_message TEXT
        )
    """)

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_posts_scheduled_at ON posts(scheduled_at)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_posts_status ON posts(status)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_posts_category ON posts(category)")

    # analyticsãƒ†ãƒ¼ãƒ–ãƒ«
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS analytics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            post_id INTEGER NOT NULL,
            threads_post_id TEXT NOT NULL,

            views INTEGER DEFAULT 0,
            likes INTEGER DEFAULT 0,
            replies INTEGER DEFAULT 0,
            reposts INTEGER DEFAULT 0,
            quotes INTEGER DEFAULT 0,

            engagement INTEGER DEFAULT 0,
            engagement_rate REAL DEFAULT 0,

            fetched_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            permalink TEXT,

            FOREIGN KEY (post_id) REFERENCES posts(id)
        )
    """)

    cursor.execute("CREATE INDEX IF NOT EXISTS idx_analytics_post_id ON analytics(post_id)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_analytics_engagement_rate ON analytics(engagement_rate)")

    # competitorsãƒ†ãƒ¼ãƒ–ãƒ«
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS competitors (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            post_url TEXT NOT NULL UNIQUE,
            threads_post_id TEXT NOT NULL,
            username TEXT,
            category TEXT,
            note TEXT,

            text TEXT,
            posted_at DATETIME,
            char_count INTEGER,
            has_emoji BOOLEAN DEFAULT 0,

            added_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            analyzed_at DATETIME
        )
    """)

    cursor.execute("CREATE INDEX IF NOT EXISTS idx_competitors_category ON competitors(category)")
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_competitors_posted_at ON competitors(posted_at)")

    conn.commit()
    conn.close()

    print(f"  âœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä½œæˆå®Œäº†: {DB_FILE}")
    print(f"  âœ“ ãƒ†ãƒ¼ãƒ–ãƒ«: posts, analytics, competitors")


def import_csv(csv_file=CSV_FILE):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰postsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    print("\n" + "="*70)
    print(f"ğŸ“¥ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­: {csv_file}")
    print("="*70)

    if not os.path.exists(csv_file):
        print(f"  âœ— ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
        return

    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    imported = 0
    skipped = 0

    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)

        for row in reader:
            csv_id = row.get('id', '').strip()
            datetime_str = row.get('datetime', '').strip()
            text = row.get('text', '').strip()
            thread_text = row.get('thread_text', '').strip() or None

            if not csv_id or not datetime_str or not text:
                print(f"  âš  ã‚¹ã‚­ãƒƒãƒ—: ä¸å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ (ID: {csv_id})")
                skipped += 1
                continue

            try:
                # æ—¥æ™‚ã®ãƒ‘ãƒ¼ã‚¹
                scheduled_at = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M')

                # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
                char_count = len(text)
                has_emoji = any(ord(c) > 127 for c in text)

                # ã‚«ãƒ†ã‚´ãƒªã®æ¨å®šï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ï¼‰
                category = detect_category(text)

                # æŒ¿å…¥
                cursor.execute("""
                    INSERT INTO posts (
                        csv_id, scheduled_at, text, thread_text, status,
                        char_count, has_emoji, category
                    ) VALUES (?, ?, ?, ?, 'pending', ?, ?, ?)
                """, (csv_id, scheduled_at, text, thread_text, char_count, has_emoji, category))

                imported += 1

            except sqlite3.IntegrityError:
                print(f"  âš  ã‚¹ã‚­ãƒƒãƒ—: æ—¢ã«å­˜åœ¨ (ID: {csv_id})")
                skipped += 1
            except Exception as e:
                print(f"  âœ— ã‚¨ãƒ©ãƒ¼ (ID: {csv_id}): {e}")
                skipped += 1

    conn.commit()
    conn.close()

    print(f"\n  âœ“ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†:")
    print(f"    - æˆåŠŸ: {imported}ä»¶")
    print(f"    - ã‚¹ã‚­ãƒƒãƒ—: {skipped}ä»¶")


def import_log(log_file=LOG_FILE):
    """posted_log.jsonã‹ã‚‰æŠ•ç¨¿æ¸ˆã¿æƒ…å ±ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    print("\n" + "="*70)
    print(f"ğŸ“¥ æŠ•ç¨¿ãƒ­ã‚°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­: {log_file}")
    print("="*70)

    if not os.path.exists(log_file):
        print(f"  âš  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {log_file}")
        return

    with open(log_file, 'r', encoding='utf-8') as f:
        log_data = json.load(f)

    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    updated = 0
    not_found = 0

    for csv_id, log_entry in log_data.items():
        threads_post_id = log_entry.get('threads_post_id')
        posted_at_str = log_entry.get('posted_at')

        if not threads_post_id or not posted_at_str:
            continue

        try:
            posted_at = datetime.fromisoformat(posted_at_str)

            # postsãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ›´æ–°
            cursor.execute("""
                UPDATE posts
                SET status = 'posted',
                    threads_post_id = ?,
                    posted_at = ?,
                    updated_at = CURRENT_TIMESTAMP
                WHERE csv_id = ?
            """, (threads_post_id, posted_at, csv_id))

            if cursor.rowcount > 0:
                updated += 1
            else:
                not_found += 1
                print(f"  âš  DBå†…ã«è¦‹ã¤ã‹ã‚‰ãªã„ (CSV ID: {csv_id})")

        except Exception as e:
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼ (CSV ID: {csv_id}): {e}")

    conn.commit()
    conn.close()

    print(f"\n  âœ“ ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†:")
    print(f"    - æ›´æ–°: {updated}ä»¶")
    print(f"    - æœªç™ºè¦‹: {not_found}ä»¶")


def detect_category(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š"""
    keywords = {
        'æ‹æ„›': ['å½¼æ°', 'å½¼å¥³', 'æ‹æ„›', 'ãƒãƒƒãƒãƒ³ã‚°', 'å‡ºä¼šã„', 'å¥½ã', 'ãƒ‡ãƒ¼ãƒˆ', 'çµå©š'],
        'ä»•äº‹': ['è»¢è·', 'ä»•äº‹', 'è·å ´', 'ä¼šç¤¾', 'ä¸Šå¸', 'çµ¦æ–™', 'æ®‹æ¥­', 'ã‚­ãƒ£ãƒªã‚¢'],
        'ãŠé‡‘': ['è²¯é‡‘', 'ãŠé‡‘', 'ç¯€ç´„', 'NISA', 'æŠ•è³‡', 'ã‚µãƒ–ã‚¹ã‚¯', 'ã‚¯ãƒ¬ã‚«', 'æµªè²»'],
        'ãƒ¡ãƒ³ã‚¿ãƒ«': ['HSP', 'ç¹Šç´°', 'è‡ªå·±è‚¯å®šæ„Ÿ', 'ã‚¹ãƒˆãƒ¬ã‚¹', 'ç–²ã‚Œ', 'ä¸å®‰', 'ãƒ¡ãƒ³ã‚¿ãƒ«'],
        'å ã„': ['å ã„', 'æœˆæ˜Ÿåº§', 'æ•°ç§˜', 'ã‚¿ãƒ­ãƒƒãƒˆ', 'ãƒ›ãƒ­ã‚¹ã‚³ãƒ¼ãƒ—', 'ã‚¹ãƒ”ãƒªãƒãƒ¥ã‚¢ãƒ«']
    }

    for category, words in keywords.items():
        if any(word in text for word in words):
            return category

    return 'ãã®ä»–'


def show_stats():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆã‚’è¡¨ç¤º"""
    print("\n" + "="*70)
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ")
    print("="*70)

    if not os.path.exists(DB_FILE):
        print(f"  âœ— ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {DB_FILE}")
        return

    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    # æŠ•ç¨¿æ•°
    cursor.execute("SELECT COUNT(*) FROM posts")
    total_posts = cursor.fetchone()[0]

    cursor.execute("SELECT COUNT(*) FROM posts WHERE status = 'pending'")
    pending_posts = cursor.fetchone()[0]

    cursor.execute("SELECT COUNT(*) FROM posts WHERE status = 'posted'")
    posted_posts = cursor.fetchone()[0]

    cursor.execute("SELECT COUNT(*) FROM posts WHERE status = 'failed'")
    failed_posts = cursor.fetchone()[0]

    print(f"\nğŸ“ æŠ•ç¨¿çµ±è¨ˆ:")
    print(f"  - ç·æ•°: {total_posts}ä»¶")
    print(f"  - æœªæŠ•ç¨¿: {pending_posts}ä»¶")
    print(f"  - æŠ•ç¨¿æ¸ˆã¿: {posted_posts}ä»¶")
    print(f"  - å¤±æ•—: {failed_posts}ä»¶")

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥
    cursor.execute("""
        SELECT category, COUNT(*)
        FROM posts
        GROUP BY category
        ORDER BY COUNT(*) DESC
    """)
    categories = cursor.fetchall()

    if categories:
        print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥:")
        for category, count in categories:
            print(f"  - {category}: {count}ä»¶")

    # ç›´è¿‘ã®æŠ•ç¨¿äºˆå®š
    cursor.execute("""
        SELECT csv_id, scheduled_at, substr(text, 1, 50)
        FROM posts
        WHERE status = 'pending'
        ORDER BY scheduled_at
        LIMIT 5
    """)
    upcoming = cursor.fetchall()

    if upcoming:
        print(f"\nğŸ“… ç›´è¿‘ã®æŠ•ç¨¿äºˆå®š:")
        for csv_id, scheduled_at, text_preview in upcoming:
            print(f"  - [{scheduled_at}] {text_preview}...")

    conn.close()


def export_to_csv(output_file='posts_export.csv'):
    """SQLiteã‹ã‚‰CSVã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    print("\n" + "="*70)
    print(f"ğŸ“¤ CSVã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­: {output_file}")
    print("="*70)

    if not os.path.exists(DB_FILE):
        print(f"  âœ— ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {DB_FILE}")
        return

    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    cursor.execute("""
        SELECT csv_id, scheduled_at, text, status, category
        FROM posts
        ORDER BY scheduled_at
    """)

    rows = cursor.fetchall()

    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['id', 'datetime', 'text', 'status', 'category'])

        for row in rows:
            csv_id, scheduled_at, text, status, category = row
            # datetimeã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            dt = datetime.fromisoformat(scheduled_at).strftime('%Y-%m-%d %H:%M')
            writer.writerow([csv_id, dt, text, status, category])

    conn.close()

    print(f"  âœ“ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {len(rows)}ä»¶")


def main():
    parser = argparse.ArgumentParser(
        description='CSV â†’ SQLite ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ„ãƒ¼ãƒ«',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
  python migrate_to_sqlite.py init

  # 2. CSVã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
  python migrate_to_sqlite.py import-csv

  # 3. posted_log.jsonã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
  python migrate_to_sqlite.py import-log

  # 4. çµ±è¨ˆç¢ºèª
  python migrate_to_sqlite.py stats

  # 5. å…¨å·¥ç¨‹ã‚’ä¸€æ‹¬å®Ÿè¡Œ
  python migrate_to_sqlite.py full

  # 6. CSVã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰
  python migrate_to_sqlite.py export --output backup.csv
        """
    )

    parser.add_argument(
        'command',
        choices=['init', 'import-csv', 'import-log', 'stats', 'export', 'full'],
        help='å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰'
    )

    parser.add_argument('--csv', type=str, default=CSV_FILE, help='CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--log', type=str, default=LOG_FILE, help='ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--output', type=str, default='posts_export.csv', help='å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«')

    args = parser.parse_args()

    if args.command == 'init':
        create_database()

    elif args.command == 'import-csv':
        import_csv(args.csv)

    elif args.command == 'import-log':
        import_log(args.log)

    elif args.command == 'stats':
        show_stats()

    elif args.command == 'export':
        export_to_csv(args.output)

    elif args.command == 'full':
        # å…¨å·¥ç¨‹ã‚’å®Ÿè¡Œ
        create_database()
        import_csv(args.csv)
        import_log(args.log)
        show_stats()
        print("\n" + "="*70)
        print("âœ… ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
        print("="*70)
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("  1. python threads_sqlite.py --dry-run  # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ã§ãƒ†ã‚¹ãƒˆ")
        print("  2. python threads_sqlite.py            # æœ¬ç•ªå®Ÿè¡Œ")

    print()


if __name__ == '__main__':
    main()
