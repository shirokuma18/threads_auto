#!/usr/bin/env python3
"""
Threads API äºˆç´„æŠ•ç¨¿ + PDCAåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
3æ—¥ã‚µã‚¤ã‚¯ãƒ«ã§PDCAã‚’å›ã™ã“ã¨ã«ç‰¹åŒ–
"""

import csv
import time
import requests
import json
from datetime import datetime, timedelta
from collections import defaultdict
import os
import sys
import argparse

# ============================================
# è¨­å®šé …ç›®
# ============================================
ACCESS_TOKEN = os.getenv('THREADS_ACCESS_TOKEN', 'YOUR_ACCESS_TOKEN_HERE')
USER_ID = os.getenv('THREADS_USER_ID', 'YOUR_USER_ID_HERE')

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
CSV_FILE = os.getenv('CSV_FILE', 'posts_schedule.csv')
LOG_FILE = 'posted_log.json'
ANALYTICS_FILE = 'analytics_data.csv'
PDCA_REPORT_FILE = 'pdca_report.md'
COMPETITORS_FILE = 'competitors.csv'
COMPETITOR_ANALYTICS_FILE = 'competitor_analytics.csv'
COMPETITOR_REPORT_FILE = 'competitor_report.md'

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
MIN_INTERVAL_SECONDS = 3600  # 1æ™‚é–“

# ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰
DRY_RUN = False

# ============================================
# Threads API
# ============================================
API_BASE_URL = 'https://graph.threads.net/v1.0'


def create_threads_post(text):
    """Threads APIã§æŠ•ç¨¿ã‚’ä½œæˆ"""
    global DRY_RUN

    if DRY_RUN:
        print(f"  â†’ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] æŠ•ç¨¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆä¸­...")
        print(f"  â†’ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] å®Ÿéš›ã«ã¯æŠ•ç¨¿ã•ã‚Œã¾ã›ã‚“")
        time.sleep(0.5)  # ãƒªã‚¢ãƒ«ãªå‹•ä½œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        fake_post_id = f"dry_run_{int(time.time())}"
        print(f"  âœ“ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] æŠ•ç¨¿æˆåŠŸï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰ï¼ (ID: {fake_post_id})")
        return fake_post_id

    try:
        create_url = f'{API_BASE_URL}/{USER_ID}/threads'
        create_params = {
            'media_type': 'TEXT',
            'text': text,
            'access_token': ACCESS_TOKEN
        }

        print(f"  â†’ ã‚³ãƒ³ãƒ†ãƒŠä½œæˆä¸­...")
        create_response = requests.post(create_url, params=create_params)
        create_response.raise_for_status()
        container_id = create_response.json().get('id')

        if not container_id:
            print(f"  âœ— ã‚³ãƒ³ãƒ†ãƒŠIDã®å–å¾—ã«å¤±æ•—")
            return None

        publish_url = f'{API_BASE_URL}/{USER_ID}/threads_publish'
        publish_params = {
            'creation_id': container_id,
            'access_token': ACCESS_TOKEN
        }

        print(f"  â†’ æŠ•ç¨¿å…¬é–‹ä¸­...")
        publish_response = requests.post(publish_url, params=publish_params)
        publish_response.raise_for_status()

        post_id = publish_response.json().get('id')
        if post_id:
            print(f"  âœ“ æŠ•ç¨¿æˆåŠŸï¼ (ID: {post_id})")
            return post_id
        else:
            print(f"  âœ— æŠ•ç¨¿IDã®å–å¾—ã«å¤±æ•—")
            return None

    except requests.exceptions.RequestException as e:
        print(f"  âœ— API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def get_post_insights(post_id):
    """æŠ•ç¨¿ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        url = f'{API_BASE_URL}/{post_id}'
        params = {
            'fields': 'id,text,timestamp,permalink',
            'access_token': ACCESS_TOKEN
        }
        
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()
        
        insights_url = f'{API_BASE_URL}/{post_id}/insights'
        insights_params = {
            'metric': 'views,likes,replies,reposts,quotes',
            'access_token': ACCESS_TOKEN
        }
        
        insights_response = requests.get(insights_url, params=insights_params)
        
        insights_data = {}
        if insights_response.status_code == 200:
            insights = insights_response.json().get('data', [])
            for metric in insights:
                insights_data[metric['name']] = metric.get('values', [{}])[0].get('value', 0)
        
        result = {
            'post_id': post_id,
            'text': data.get('text', ''),
            'timestamp': data.get('timestamp', ''),
            'permalink': data.get('permalink', ''),
            'views': insights_data.get('views', 0),
            'likes': insights_data.get('likes', 0),
            'replies': insights_data.get('replies', 0),
            'reposts': insights_data.get('reposts', 0),
            'quotes': insights_data.get('quotes', 0),
        }
        
        total_engagement = (
            result['likes'] + 
            result['replies'] + 
            result['reposts'] + 
            result['quotes']
        )
        result['engagement'] = total_engagement
        result['engagement_rate'] = (
            (total_engagement / result['views'] * 100) 
            if result['views'] > 0 else 0
        )
        
        return result
        
    except requests.exceptions.RequestException as e:
        print(f"  âœ— åˆ†æãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


# ============================================
# ç«¶åˆåˆ†ææ©Ÿèƒ½
# ============================================

def extract_post_id_from_url(url):
    """
    ThreadsæŠ•ç¨¿URLã‹ã‚‰æŠ•ç¨¿IDã‚’æŠ½å‡º

    Args:
        url: ThreadsæŠ•ç¨¿URL (ä¾‹: https://www.threads.net/@username/post/ABC123...)

    Returns:
        æŠ•ç¨¿IDã€ã¾ãŸã¯None
    """
    import re

    # URLã®ãƒ‘ã‚¿ãƒ¼ãƒ³: https://www.threads.net/@username/post/{POST_ID}
    pattern = r'threads\.net/@[^/]+/post/([A-Za-z0-9_-]+)'
    match = re.search(pattern, url)

    if match:
        return match.group(1)

    return None


def load_competitor_posts():
    """ç«¶åˆæŠ•ç¨¿URLãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    posts = []
    try:
        with open(COMPETITORS_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                url = row.get('post_url', '').strip()

                # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                if url.startswith('#') or not url:
                    continue

                # URLã‹ã‚‰æŠ•ç¨¿IDã‚’æŠ½å‡º
                post_id = extract_post_id_from_url(url)
                if post_id:
                    posts.append({
                        'post_id': post_id,
                        'url': url,
                        'category': row.get('category', ''),
                        'note': row.get('note', '')
                    })
                else:
                    print(f"è­¦å‘Š: ç„¡åŠ¹ãªURLå½¢å¼: {url}")

    except FileNotFoundError:
        print(f"è­¦å‘Š: {COMPETITORS_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    return posts


def get_thread_details(thread_id):
    """
    æŠ•ç¨¿ã®è©³ç´°æƒ…å ±ã‚’å–å¾—

    Args:
        thread_id: æŠ•ç¨¿ID

    Returns:
        æŠ•ç¨¿ã®è©³ç´°æƒ…å ±
    """
    try:
        url = f'{API_BASE_URL}/{thread_id}'
        params = {
            'fields': 'id,text,timestamp,permalink,media_type,media_url,username',
            'access_token': ACCESS_TOKEN
        }

        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        # Threads APIã§ã¯å…¬é–‹ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ•°ã®å–å¾—ãŒåˆ¶é™ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€
        # åŸºæœ¬æƒ…å ±ã®ã¿ã‚’å–å¾—
        result = {
            'thread_id': thread_id,
            'text': data.get('text', ''),
            'timestamp': data.get('timestamp', ''),
            'permalink': data.get('permalink', ''),
            'username': data.get('username', ''),
            'media_type': data.get('media_type', 'TEXT'),
            'char_count': len(data.get('text', ''))
        }

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰æ™‚é–“å¸¯ã¨æ›œæ—¥ã‚’æŠ½å‡º
        if result['timestamp']:
            posted_time = datetime.fromisoformat(result['timestamp'].replace('Z', '+00:00'))
            result['posted_hour'] = posted_time.hour
            result['posted_day'] = posted_time.strftime('%A')
            result['posted_datetime'] = posted_time

        # çµµæ–‡å­—ã®æœ‰ç„¡
        text = result.get('text', '')
        result['has_emoji'] = any(ord(c) > 127 for c in text)

        return result

    except requests.exceptions.RequestException as e:
        print(f"  âœ— æŠ•ç¨¿è©³ç´°å–å¾—ã‚¨ãƒ©ãƒ¼ (ID: {thread_id}): {e}")
        return None


def save_competitor_analytics(analytics_data):
    """ç«¶åˆåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜"""
    file_exists = os.path.exists(COMPETITOR_ANALYTICS_FILE)

    fieldnames = [
        'timestamp', 'competitor_username', 'thread_id', 'text',
        'posted_hour', 'posted_day', 'char_count', 'has_emoji',
        'media_type', 'permalink'
    ]

    with open(COMPETITOR_ANALYTICS_FILE, 'a', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)

        if not file_exists:
            writer.writeheader()

        writer.writerow({
            'timestamp': datetime.now().isoformat(),
            'competitor_username': analytics_data.get('username', ''),
            'thread_id': analytics_data.get('thread_id', ''),
            'text': analytics_data.get('text', '')[:200],
            'posted_hour': analytics_data.get('posted_hour', ''),
            'posted_day': analytics_data.get('posted_day', ''),
            'char_count': analytics_data.get('char_count', 0),
            'has_emoji': analytics_data.get('has_emoji', False),
            'media_type': analytics_data.get('media_type', 'TEXT'),
            'permalink': analytics_data.get('permalink', '')
        })


# ============================================
# CSV & ãƒ­ã‚°ç®¡ç†
# ============================================

def load_schedule():
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ•ç¨¿ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    posts = []
    try:
        with open(CSV_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                posts.append({
                    'id': row.get('id', ''),
                    'datetime': row.get('datetime', ''),
                    'text': row.get('text', '')
                })
    except FileNotFoundError:
        print(f"è­¦å‘Š: {CSV_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    return posts


def load_posted_log():
    """æŠ•ç¨¿æ¸ˆã¿ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_posted_log(log):
    """æŠ•ç¨¿æ¸ˆã¿ãƒ­ã‚°ã‚’ä¿å­˜"""
    with open(LOG_FILE, 'w', encoding='utf-8') as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


def save_analytics_to_csv(analytics_data):
    """åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜"""
    file_exists = os.path.exists(ANALYTICS_FILE)
    
    fieldnames = [
        'timestamp', 'post_id', 'text', 'posted_hour', 'posted_day',
        'views', 'likes', 'replies', 'reposts', 'quotes', 
        'engagement', 'engagement_rate', 'char_count', 'has_emoji'
    ]
    
    with open(ANALYTICS_FILE, 'a', encoding='utf-8', newline='') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        
        if not file_exists:
            writer.writeheader()
        
        # æŠ•ç¨¿æ™‚é–“ã‹ã‚‰æ™‚é–“å¸¯ã¨æ›œæ—¥ã‚’æŠ½å‡º
        posted_time = datetime.fromisoformat(analytics_data.get('timestamp', '').replace('Z', '+00:00'))
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
        text = analytics_data.get('text', '')
        has_emoji = any(ord(c) > 127 for c in text)  # ç°¡æ˜“çš„ãªçµµæ–‡å­—åˆ¤å®š
        
        writer.writerow({
            'timestamp': datetime.now().isoformat(),
            'post_id': analytics_data.get('post_id', ''),
            'text': text[:100],
            'posted_hour': posted_time.hour,
            'posted_day': posted_time.strftime('%A'),
            'views': analytics_data.get('views', 0),
            'likes': analytics_data.get('likes', 0),
            'replies': analytics_data.get('replies', 0),
            'reposts': analytics_data.get('reposts', 0),
            'quotes': analytics_data.get('quotes', 0),
            'engagement': analytics_data.get('engagement', 0),
            'engagement_rate': f"{analytics_data.get('engagement_rate', 0):.2f}",
            'char_count': len(text),
            'has_emoji': has_emoji
        })


# ============================================
# PDCAåˆ†ææ©Ÿèƒ½
# ============================================

def generate_3day_report():
    """
    éå»3æ—¥é–“ã®PDCAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    """
    print("\n" + "="*70)
    print("ğŸ“Š éå»3æ—¥é–“ã®PDCAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    print("="*70)
    
    # éå»3æ—¥é–“ã®æŠ•ç¨¿ã‚’å–å¾—
    posted_log = load_posted_log()
    three_days_ago = datetime.now() - timedelta(days=3)
    
    recent_posts = []
    for post_id, post_data in posted_log.items():
        posted_at = datetime.fromisoformat(post_data.get('posted_at', ''))
        if posted_at >= three_days_ago and 'threads_post_id' in post_data:
            recent_posts.append({
                'csv_id': post_id,
                'threads_id': post_data['threads_post_id'],
                'text': post_data['text'],
                'posted_at': posted_at,
                'scheduled_at': post_data.get('scheduled_at', '')
            })
    
    if not recent_posts:
        print("âš ï¸  éå»3æ—¥é–“ã®æŠ•ç¨¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    print(f"\néå»3æ—¥é–“ã®æŠ•ç¨¿æ•°: {len(recent_posts)}ä»¶")
    print("åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\n")
    
    # å„æŠ•ç¨¿ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    analytics_results = []
    for i, post in enumerate(recent_posts, 1):
        print(f"[{i}/{len(recent_posts)}] æŠ•ç¨¿ {post['csv_id']} ã‚’åˆ†æä¸­...")
        analytics = get_post_insights(post['threads_id'])
        if analytics:
            analytics['posted_at'] = post['posted_at']
            analytics['csv_id'] = post['csv_id']
            analytics_results.append(analytics)
            save_analytics_to_csv(analytics)
            print(f"  âœ“ å®Œäº†")
        time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
    
    if not analytics_results:
        print("âš ï¸  åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = generate_pdca_markdown(analytics_results)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(PDCA_REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nâœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ {PDCA_REPORT_FILE} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print("\n" + report)
    
    return report


def generate_pdca_markdown(analytics_results):
    """
    PDCAãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownå½¢å¼ã§ç”Ÿæˆ
    """
    # åŸºæœ¬çµ±è¨ˆ
    total_posts = len(analytics_results)
    total_views = sum(a['views'] for a in analytics_results)
    total_engagement = sum(a['engagement'] for a in analytics_results)
    avg_engagement_rate = sum(a['engagement_rate'] for a in analytics_results) / total_posts
    
    # æ™‚é–“å¸¯åˆ¥åˆ†æ
    hourly_performance = defaultdict(lambda: {'count': 0, 'engagement_rate': 0})
    for a in analytics_results:
        hour = a['posted_at'].hour
        hourly_performance[hour]['count'] += 1
        hourly_performance[hour]['engagement_rate'] += a['engagement_rate']
    
    for hour in hourly_performance:
        hourly_performance[hour]['avg_engagement_rate'] = (
            hourly_performance[hour]['engagement_rate'] / 
            hourly_performance[hour]['count']
        )
    
    # ãƒ™ã‚¹ãƒˆæ™‚é–“å¸¯
    best_hours = sorted(
        hourly_performance.items(),
        key=lambda x: x[1]['avg_engagement_rate'],
        reverse=True
    )[:3]
    
    # ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿
    top_posts = sorted(analytics_results, key=lambda x: x['engagement_rate'], reverse=True)[:3]
    worst_posts = sorted(analytics_results, key=lambda x: x['engagement_rate'])[:3]
    
    # æ–‡å­—æ•°åˆ†æ
    char_counts = [len(a['text']) for a in analytics_results]
    avg_char_count = sum(char_counts) / len(char_counts) if char_counts else 0
    
    # çµµæ–‡å­—ä½¿ç”¨ã®åŠ¹æœ
    with_emoji = [a for a in analytics_results if any(ord(c) > 127 for c in a['text'])]
    without_emoji = [a for a in analytics_results if not any(ord(c) > 127 for c in a['text'])]
    
    emoji_effect = ""
    if with_emoji and without_emoji:
        avg_with_emoji = sum(a['engagement_rate'] for a in with_emoji) / len(with_emoji)
        avg_without_emoji = sum(a['engagement_rate'] for a in without_emoji) / len(without_emoji)
        emoji_diff = avg_with_emoji - avg_without_emoji
        emoji_effect = f"çµµæ–‡å­—ã‚ã‚Š: {avg_with_emoji:.2f}% vs ãªã—: {avg_without_emoji:.2f}% (å·®: {emoji_diff:+.2f}%)"
    
    # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    now = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')
    
    report = f"""# ğŸ“Š Threads PDCA ãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {now}  
**åˆ†ææœŸé–“**: éå»3æ—¥é–“  
**æŠ•ç¨¿æ•°**: {total_posts}ä»¶

---

## ğŸ“ˆ ã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| **ç·è¡¨ç¤ºå›æ•°** | {total_views:,} |
| **ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ** | {total_engagement:,} |
| **å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡** | {avg_engagement_rate:.2f}% |
| **å¹³å‡æ–‡å­—æ•°** | {avg_char_count:.0f}æ–‡å­— |

---

## ğŸ† ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿

"""
    
    for i, post in enumerate(top_posts, 1):
        report += f"""### {i}ä½: {post['engagement_rate']:.2f}%

- **ãƒ†ã‚­ã‚¹ãƒˆ**: {post['text'][:80]}...
- **è¡¨ç¤ºå›æ•°**: {post['views']:,}
- **ã„ã„ã­**: {post['likes']:,} | **è¿”ä¿¡**: {post['replies']:,} | **ãƒªãƒã‚¹ãƒˆ**: {post['reposts']:,}
- **æŠ•ç¨¿æ™‚åˆ»**: {post['posted_at'].strftime('%m/%d %H:%M')}

"""
    
    report += f"""---

## â° ãƒ™ã‚¹ãƒˆæŠ•ç¨¿æ™‚é–“å¸¯

"""
    
    for i, (hour, data) in enumerate(best_hours, 1):
        report += f"{i}. **{hour:02d}æ™‚å°**: ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ {data['avg_engagement_rate']:.2f}% (æŠ•ç¨¿æ•°: {data['count']}ä»¶)\n"
    
    report += f"""

---

## ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ

### æ–‡å­—æ•°
- **å¹³å‡æ–‡å­—æ•°**: {avg_char_count:.0f}æ–‡å­—
- **æœ€çŸ­**: {min(char_counts)}æ–‡å­— | **æœ€é•·**: {max(char_counts)}æ–‡å­—

### çµµæ–‡å­—ã®åŠ¹æœ
{emoji_effect if emoji_effect else "ãƒ‡ãƒ¼ã‚¿ä¸è¶³"}

---

## âš ï¸ æ”¹å–„ãŒå¿…è¦ãªæŠ•ç¨¿

"""
    
    for i, post in enumerate(worst_posts, 1):
        report += f"""{i}. **{post['engagement_rate']:.2f}%** - {post['text'][:60]}...
"""
    
    report += f"""

---

## ğŸ’¡ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ (Plan)

### âœ… ç¶šã‘ã‚‹ã“ã¨ (Keep)

"""
    
    # æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
    if best_hours:
        best_hour = best_hours[0][0]
        report += f"1. **{best_hour:02d}æ™‚å°ã®æŠ•ç¨¿ã‚’å¢—ã‚„ã™** - æœ€ã‚‚åå¿œãŒè‰¯ã„æ™‚é–“å¸¯ã§ã™\n"
    
    if top_posts:
        top_post = top_posts[0]
        if len(top_post['text']) < 100:
            report += f"2. **çŸ­ã‚ã®æŠ•ç¨¿ãŒå¥½èª¿** - {len(top_post['text'])}æ–‡å­—ç¨‹åº¦ãŒåŠ¹æœçš„\n"
        else:
            report += f"2. **é•·æ–‡æŠ•ç¨¿ãŒå¥½èª¿** - {len(top_post['text'])}æ–‡å­—ç¨‹åº¦ã®è©³ç´°ãªå†…å®¹ãŒåŠ¹æœçš„\n"
    
    if emoji_effect and "+" in emoji_effect:
        report += "3. **çµµæ–‡å­—ã®ä½¿ç”¨ã‚’ç¶™ç¶š** - ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãŒé«˜ã„å‚¾å‘\n"
    
    report += f"""

### ğŸ”„ æ”¹å–„ã™ã‚‹ã“ã¨ (Improve)

"""
    
    # æ”¹å–„ææ¡ˆ
    if worst_posts:
        report += f"1. **ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã®åˆ†æ** - ãªãœåå¿œãŒæ‚ªã‹ã£ãŸã®ã‹æŒ¯ã‚Šè¿”ã‚‹\n"
    
    if best_hours and len(best_hours) > 0:
        avoid_hours = [h for h in range(24) if h not in [bh[0] for bh in best_hours[:5]]]
        if avoid_hours:
            report += f"2. **æŠ•ç¨¿æ™‚é–“ã®æœ€é©åŒ–** - {avoid_hours[0]:02d}æ™‚å°ãªã©ã¯é¿ã‘ã‚‹\n"
    
    report += f"""

### ğŸ†• è©¦ã™ã“ã¨ (Try)

1. **æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’è©¦ã™** - è³ªå•å½¢å¼ã€æŠ•ç¥¨ã€ãƒªã‚¹ãƒˆå½¢å¼ãªã©
2. **ç•°ãªã‚‹æ™‚é–“å¸¯ã«ãƒ†ã‚¹ãƒˆæŠ•ç¨¿** - ã¾ã è©¦ã—ã¦ã„ãªã„æ™‚é–“å¸¯ã‚’æ¢ã‚‹
3. **ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®åŠ¹æœã‚’æ¤œè¨¼** - ä½¿ç”¨æœ‰ç„¡ã§A/Bãƒ†ã‚¹ãƒˆ

---

## ğŸ“… æ¬¡ã®3æ—¥é–“ã®æŠ•ç¨¿ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ææ¡ˆ

æ¨å¥¨æŠ•ç¨¿æ™‚åˆ»:
"""
    
    # æ¬¡ã®3æ—¥é–“ã®æ¨å¥¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
    for day in range(1, 4):
        date = (datetime.now() + timedelta(days=day)).strftime('%Y-%m-%d')
        if best_hours:
            for hour, _ in best_hours[:2]:  # ãƒˆãƒƒãƒ—2ã®æ™‚é–“å¸¯
                report += f"- {date} {hour:02d}:00\n"
    
    report += f"""

---

## ğŸ”— è©³ç´°ãƒ‡ãƒ¼ã‚¿

è©³ç´°ãªåˆ†æãƒ‡ãƒ¼ã‚¿ã¯ `{ANALYTICS_FILE}` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

**æ¬¡å›ãƒ¬ãƒãƒ¼ãƒˆ**: 3æ—¥å¾Œ

"""
    
    return report


def analyze_competitors():
    """
    ç«¶åˆæŠ•ç¨¿URLã‹ã‚‰æŠ•ç¨¿ã‚’åˆ†æ
    """
    print("\n" + "="*70)
    print("ğŸ” ç«¶åˆæŠ•ç¨¿åˆ†æã‚’å®Ÿè¡Œä¸­...")
    print("="*70)

    # ç«¶åˆæŠ•ç¨¿URLãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€
    competitor_posts = load_competitor_posts()

    if not competitor_posts:
        print("âš ï¸  ç«¶åˆæŠ•ç¨¿URLãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(f"   {COMPETITORS_FILE} ã«æŠ•ç¨¿URLã‚’è¿½åŠ ã—ã¦ãã ã•ã„")
        print("\nURLã®è¿½åŠ æ–¹æ³•:")
        print("1. Threadsã‚¢ãƒ—ãƒªã§æ°—ã«ãªã‚‹æŠ•ç¨¿ã‚’é–‹ã")
        print("2. å³ä¸Šã®ã€Œ...ã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰ã€Œãƒªãƒ³ã‚¯ã‚’ã‚³ãƒ”ãƒ¼ã€")
        print(f"3. {COMPETITORS_FILE} ã«è²¼ã‚Šä»˜ã‘")
        return

    print(f"\nåˆ†æå¯¾è±¡æŠ•ç¨¿æ•°: {len(competitor_posts)}ä»¶\n")

    all_threads = []
    success_count = 0
    fail_count = 0

    for i, post_data in enumerate(competitor_posts, 1):
        post_id = post_data['post_id']
        category = post_data['category']

        print(f"[{i}/{len(competitor_posts)}] æŠ•ç¨¿ID: {post_id[:20]}... ã‚’åˆ†æä¸­...")

        # æŠ•ç¨¿ã®è©³ç´°ã‚’å–å¾—
        details = get_thread_details(post_id)

        if details:
            details['competitor_category'] = category
            details['note'] = post_data['note']
            all_threads.append(details)
            save_competitor_analytics(details)
            success_count += 1
            print(f"  âœ“ å–å¾—æˆåŠŸ")
        else:
            fail_count += 1
            print(f"  âœ— å–å¾—å¤±æ•—ï¼ˆAPIã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")

        time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

    print(f"\n{'='*70}")
    print(f"åˆ†æå®Œäº†: æˆåŠŸ {success_count}ä»¶ / å¤±æ•— {fail_count}ä»¶")
    print(f"{'='*70}\n")

    if not all_threads:
        print("âš ï¸  æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        print("\nã€åŸå› ã€‘")
        print("Threads APIã®åˆ¶é™ã«ã‚ˆã‚Šã€è‡ªåˆ†ã®ã‚¢ã‚¯ã‚»ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³ã§ã¯")
        print("ä»–äººã®æŠ•ç¨¿ã‚’å–å¾—ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        print("\nã€ä»£æ›¿æ¡ˆã€‘")
        print("1. æ‰‹å‹•ã§ç«¶åˆã®æŠ•ç¨¿ã‚’è¨˜éŒ²ã™ã‚‹")
        print("2. è‡ªåˆ†ã®æŠ•ç¨¿ã®ã¿ã§PDCAåˆ†æã‚’è¡Œã†")
        print("3. å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æŠ•ç¨¿çµ±è¨ˆãƒ„ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹")
        return

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = generate_competitor_report(all_threads)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(COMPETITOR_REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ {COMPETITOR_REPORT_FILE} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print(f"   è©³ç´°ãƒ‡ãƒ¼ã‚¿: {COMPETITOR_ANALYTICS_FILE}\n")

    return report


def generate_competitor_report(threads):
    """
    ç«¶åˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownå½¢å¼ã§ç”Ÿæˆ

    Args:
        threads: ç«¶åˆã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ

    Returns:
        Markdownãƒ¬ãƒãƒ¼ãƒˆ
    """
    total_threads = len(threads)

    # æ™‚é–“å¸¯åˆ¥åˆ†æ
    hourly_distribution = defaultdict(int)
    for thread in threads:
        hour = thread.get('posted_hour', 0)
        hourly_distribution[hour] += 1

    # æ–‡å­—æ•°åˆ†æ
    char_counts = [thread.get('char_count', 0) for thread in threads]
    avg_char_count = sum(char_counts) / len(char_counts) if char_counts else 0

    # çµµæ–‡å­—ä½¿ç”¨ç‡
    with_emoji = [t for t in threads if t.get('has_emoji', False)]
    emoji_rate = (len(with_emoji) / total_threads * 100) if total_threads > 0 else 0

    # æ›œæ—¥åˆ¥åˆ†å¸ƒ
    day_distribution = defaultdict(int)
    for thread in threads:
        day = thread.get('posted_day', 'Unknown')
        day_distribution[day] += 1

    # ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
    media_distribution = defaultdict(int)
    for thread in threads:
        media_type = thread.get('media_type', 'TEXT')
        media_distribution[media_type] += 1

    # æœ€ã‚‚æ´»ç™ºãªæ™‚é–“å¸¯
    top_hours = sorted(hourly_distribution.items(), key=lambda x: x[1], reverse=True)[:5]

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    category_stats = defaultdict(lambda: {'count': 0, 'avg_char': 0, 'total_char': 0})
    for thread in threads:
        category = thread.get('competitor_category', 'ãã®ä»–')
        category_stats[category]['count'] += 1
        category_stats[category]['total_char'] += thread.get('char_count', 0)

    for category in category_stats:
        if category_stats[category]['count'] > 0:
            category_stats[category]['avg_char'] = (
                category_stats[category]['total_char'] / category_stats[category]['count']
            )

    # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    now = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')

    report = f"""# ğŸ” ç«¶åˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {now}
**åˆ†ææŠ•ç¨¿æ•°**: {total_threads}ä»¶
**åˆ†ææœŸé–“**: éå»7æ—¥é–“ç¨‹åº¦

---

## ğŸ“Š å…¨ä½“ã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| **ç·æŠ•ç¨¿æ•°** | {total_threads}ä»¶ |
| **å¹³å‡æ–‡å­—æ•°** | {avg_char_count:.0f}æ–‡å­— |
| **çµµæ–‡å­—ä½¿ç”¨ç‡** | {emoji_rate:.1f}% |

---

## â° æŠ•ç¨¿æ™‚é–“å¸¯ã®å‚¾å‘

### ãƒˆãƒƒãƒ—5æ™‚é–“å¸¯

"""

    for i, (hour, count) in enumerate(top_hours, 1):
        percentage = (count / total_threads * 100) if total_threads > 0 else 0
        report += f"{i}. **{hour:02d}æ™‚å°**: {count}ä»¶ ({percentage:.1f}%)\n"

    report += f"""

### æ™‚é–“å¸¯åˆ¥åˆ†å¸ƒ

"""

    # æ™‚é–“å¸¯ã‚’4ã¤ã«åˆ†ã‘ã‚‹
    time_blocks = {
        'æœ (6-11æ™‚)': sum(hourly_distribution[h] for h in range(6, 12)),
        'æ˜¼ (12-17æ™‚)': sum(hourly_distribution[h] for h in range(12, 18)),
        'å¤œ (18-23æ™‚)': sum(hourly_distribution[h] for h in range(18, 24)),
        'æ·±å¤œ (0-5æ™‚)': sum(hourly_distribution[h] for h in range(0, 6))
    }

    for block, count in time_blocks.items():
        percentage = (count / total_threads * 100) if total_threads > 0 else 0
        report += f"- {block}: {count}ä»¶ ({percentage:.1f}%)\n"

    report += f"""

---

## ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ

### æ–‡å­—æ•°

- **å¹³å‡**: {avg_char_count:.0f}æ–‡å­—
- **æœ€çŸ­**: {min(char_counts) if char_counts else 0}æ–‡å­—
- **æœ€é•·**: {max(char_counts) if char_counts else 0}æ–‡å­—

### çµµæ–‡å­—ã®ä½¿ç”¨

- **ä½¿ç”¨ç‡**: {emoji_rate:.1f}% ({len(with_emoji)}/{total_threads}ä»¶)

### ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—

"""

    for media_type, count in sorted(media_distribution.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / total_threads * 100) if total_threads > 0 else 0
        report += f"- **{media_type}**: {count}ä»¶ ({percentage:.1f}%)\n"

    report += f"""

---

## ğŸ“… æ›œæ—¥åˆ¥ã®å‚¾å‘

"""

    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
    day_names = {
        'Monday': 'æœˆæ›œæ—¥',
        'Tuesday': 'ç«æ›œæ—¥',
        'Wednesday': 'æ°´æ›œæ—¥',
        'Thursday': 'æœ¨æ›œæ—¥',
        'Friday': 'é‡‘æ›œæ—¥',
        'Saturday': 'åœŸæ›œæ—¥',
        'Sunday': 'æ—¥æ›œæ—¥'
    }

    for day in day_order:
        count = day_distribution.get(day, 0)
        if total_threads > 0:
            percentage = (count / total_threads * 100)
            report += f"- {day_names.get(day, day)}: {count}ä»¶ ({percentage:.1f}%)\n"

    report += f"""

---

## ğŸ¯ ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ

"""

    for category, stats in sorted(category_stats.items(), key=lambda x: x[1]['count'], reverse=True):
        report += f"""### {category}

- æŠ•ç¨¿æ•°: {stats['count']}ä»¶
- å¹³å‡æ–‡å­—æ•°: {stats['avg_char']:.0f}æ–‡å­—

"""

    report += f"""---

## ğŸ’¡ ã‚¤ãƒ³ã‚µã‚¤ãƒˆã¨æ¨å¥¨äº‹é …

### ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³

"""

    # ã‚¤ãƒ³ã‚µã‚¤ãƒˆã®è‡ªå‹•ç”Ÿæˆ
    if top_hours:
        top_hour = top_hours[0][0]
        report += f"1. **äººæ°—ã®æŠ•ç¨¿æ™‚é–“å¸¯**: {top_hour:02d}æ™‚å°ãŒæœ€ã‚‚æŠ•ç¨¿ãŒå¤šã„\n"

    if emoji_rate > 50:
        report += f"2. **çµµæ–‡å­—ã®æ´»ç”¨**: {emoji_rate:.0f}%ã®æŠ•ç¨¿ã§çµµæ–‡å­—ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹\n"
    elif emoji_rate < 20:
        report += f"2. **çµµæ–‡å­—ä½¿ç”¨ã¯æ§ãˆã‚**: {emoji_rate:.0f}%ã®ã¿ãŒçµµæ–‡å­—ã‚’ä½¿ç”¨\n"

    if avg_char_count < 100:
        report += f"3. **çŸ­æ–‡æŠ•ç¨¿ãŒä¸»æµ**: å¹³å‡{avg_char_count:.0f}æ–‡å­—ã®ç°¡æ½”ãªæŠ•ç¨¿\n"
    elif avg_char_count > 200:
        report += f"3. **è©³ç´°ãªæŠ•ç¨¿**: å¹³å‡{avg_char_count:.0f}æ–‡å­—ã®é•·æ–‡æŠ•ç¨¿\n"

    # æ™‚é–“å¸¯åˆ¥ã®æ¨å¥¨
    if time_blocks['å¤œ (18-23æ™‚)'] > time_blocks['æœ (6-11æ™‚)']:
        report += f"4. **å¤œé–“ã®æŠ•ç¨¿ãŒå¤šã„**: 18-23æ™‚å°ã®æŠ•ç¨¿ãŒæ´»ç™º\n"

    report += f"""

### ã‚ãªãŸã®æŠ•ç¨¿æˆ¦ç•¥ã¸ã®ææ¡ˆ

"""

    # ææ¡ˆã‚’ç”Ÿæˆ
    if top_hours:
        report += f"1. **æŠ•ç¨¿æ™‚é–“**: {top_hours[0][0]:02d}æ™‚å°ã‚’è©¦ã—ã¦ã¿ã‚‹\n"

    report += f"2. **æ–‡å­—æ•°**: {avg_char_count:.0f}æ–‡å­—å‰å¾Œã‚’ç›®å®‰ã«ã™ã‚‹\n"

    if emoji_rate > 50:
        report += f"3. **çµµæ–‡å­—**: ç«¶åˆã®å¤šããŒçµµæ–‡å­—ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€æ´»ç”¨ã‚’æ¤œè¨\n"

    report += f"""

---

## ğŸ“š ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿

ä»¥ä¸‹ã¯åˆ†æå¯¾è±¡ã®æŠ•ç¨¿ä¾‹ã§ã™:

"""

    # ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿ã‚’5ä»¶è¡¨ç¤º
    for i, thread in enumerate(threads[:5], 1):
        text = thread.get('text', '')[:100]
        hour = thread.get('posted_hour', 0)
        char_count = thread.get('char_count', 0)
        username = thread.get('username', 'ä¸æ˜')

        report += f"""### ã‚µãƒ³ãƒ—ãƒ« {i}

- **æŠ•ç¨¿è€…**: @{username}
- **æŠ•ç¨¿æ™‚åˆ»**: {hour:02d}æ™‚å°
- **æ–‡å­—æ•°**: {char_count}æ–‡å­—
- **å†…å®¹**: {text}...

"""

    report += f"""---

## ğŸ”— è©³ç´°ãƒ‡ãƒ¼ã‚¿

è©³ç´°ãªåˆ†æãƒ‡ãƒ¼ã‚¿ã¯ `{COMPETITOR_ANALYTICS_FILE}` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

**æ¬¡å›åˆ†æ**: 1é€±é–“å¾Œ

"""

    return report


def suggest_next_posts():
    """
    æ¬¡ã®æŠ•ç¨¿IDã¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ææ¡ˆ
    """
    posts = load_schedule()
    
    if not posts:
        next_id = 1
    else:
        max_id = max(int(p['id']) for p in posts if p['id'].isdigit())
        next_id = max_id + 1
    
    # éå»ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€é©ãªæ™‚é–“ã‚’ææ¡ˆ
    best_times = []
    if os.path.exists(ANALYTICS_FILE):
        with open(ANALYTICS_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            hourly_rates = defaultdict(list)
            for row in reader:
                try:
                    hour = int(row['posted_hour'])
                    rate = float(row['engagement_rate'])
                    hourly_rates[hour].append(rate)
                except:
                    continue
            
            # å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãŒé«˜ã„æ™‚é–“å¸¯
            avg_rates = {h: sum(rates)/len(rates) for h, rates in hourly_rates.items()}
            best_times = sorted(avg_rates.items(), key=lambda x: x[1], reverse=True)[:3]
    
    print("\n" + "="*70)
    print("ğŸ“ æ¬¡ã®æŠ•ç¨¿ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ")
    print("="*70)
    print(f"\næ¬¡ã®ID: {next_id}")
    
    if best_times:
        print("\næ¨å¥¨æŠ•ç¨¿æ™‚é–“:")
        for hour, rate in best_times:
            print(f"  - {hour:02d}:00 (å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡: {rate:.2f}%)")
    
    # CSVãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
    tomorrow = datetime.now() + timedelta(days=1)
    csv_template = "\n# æ¬¡ã®æŠ•ç¨¿ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ï¼ˆä»¥ä¸‹ã®è¡Œã‚’ã‚³ãƒ”ãƒ¼ï¼‰\n"
    
    for i in range(3):  # 3æ—¥åˆ†
        date = (tomorrow + timedelta(days=i)).strftime('%Y-%m-%d')
        if best_times:
            for j, (hour, _) in enumerate(best_times[:2]):  # 1æ—¥2å›
                csv_template += f"{next_id + i*2 + j},{date} {hour:02d}:00,ã“ã“ã«æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›\n"
        else:
            csv_template += f"{next_id + i},{date} 09:00,ã“ã“ã«æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›\n"
            csv_template += f"{next_id + i + 1},{date} 18:00,ã“ã“ã«æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›\n"
    
    print(csv_template)
    print("\n" + "="*70)


# ============================================
# æŠ•ç¨¿å‡¦ç†
# ============================================

def check_and_post():
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æŠ•ç¨¿"""
    global DRY_RUN

    posts = load_schedule()
    posted_log = load_posted_log()
    current_time = datetime.now()

    posts_to_do = []

    for post in posts:
        post_id = post['id']

        if post_id in posted_log:
            continue

        try:
            scheduled_time = datetime.strptime(post['datetime'], '%Y-%m-%d %H:%M')
        except ValueError:
            print(f"è­¦å‘Š: ç„¡åŠ¹ãªæ—¥æ™‚å½¢å¼ (ID: {post_id}): {post['datetime']}")
            continue

        if current_time >= scheduled_time:
            posts_to_do.append({
                'id': post_id,
                'scheduled_time': scheduled_time,
                'text': post['text']
            })

    posts_to_do.sort(key=lambda x: x['scheduled_time'])

    if not posts_to_do:
        print("æŠ•ç¨¿å¾…ã¡ã®é …ç›®ã¯ã‚ã‚Šã¾ã›ã‚“")
        return

    print(f"\næŠ•ç¨¿å¾…ã¡: {len(posts_to_do)}ä»¶")

    if DRY_RUN:
        print("\n[ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] ä»¥ä¸‹ã®æŠ•ç¨¿ãŒå®Ÿè¡Œã•ã‚Œã¾ã™:\n")

    for i, post in enumerate(posts_to_do):
        post_id = post['id']
        text = post['text']

        print(f"\n[{i+1}/{len(posts_to_do)}] æŠ•ç¨¿ID: {post_id}")
        print(f"  ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«: {post['scheduled_time'].strftime('%Y-%m-%d %H:%M')}")
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text[:80]}{'...' if len(text) > 80 else ''}")

        if DRY_RUN:
            print(f"  â†’ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] å®Ÿéš›ã«ã¯æŠ•ç¨¿ã•ã‚Œã¾ã›ã‚“")
            continue

        threads_post_id = create_threads_post(text)

        if threads_post_id:
            posted_log[post_id] = {
                'posted_at': datetime.now().isoformat(),
                'scheduled_at': post['scheduled_time'].isoformat(),
                'text': text,
                'threads_post_id': threads_post_id
            }
            save_posted_log(posted_log)

        if i < len(posts_to_do) - 1:
            wait_time = MIN_INTERVAL_SECONDS
            print(f"  â†’ æ¬¡ã®æŠ•ç¨¿ã¾ã§ {wait_time}ç§’å¾…æ©Ÿ...")
            time.sleep(wait_time)


# ============================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================

def validate_config():
    """è¨­å®šã®æ¤œè¨¼"""
    global DRY_RUN

    print("\nâœ“ ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª")

    has_token = ACCESS_TOKEN != 'YOUR_ACCESS_TOKEN_HERE'
    has_user_id = USER_ID != 'YOUR_USER_ID_HERE'

    if not has_token:
        if DRY_RUN:
            print("  âš  THREADS_ACCESS_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        else:
            print("  âœ— THREADS_ACCESS_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
    else:
        # ãƒˆãƒ¼ã‚¯ãƒ³ã®æœ€åˆã®æ•°æ–‡å­—ã ã‘è¡¨ç¤º
        masked_token = ACCESS_TOKEN[:8] + "..." if len(ACCESS_TOKEN) > 8 else "***"
        print(f"  âœ“ ACCESS_TOKEN: è¨­å®šæ¸ˆã¿ ({masked_token})")

    if not has_user_id:
        if DRY_RUN:
            print("  âš  THREADS_USER_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        else:
            print("  âœ— THREADS_USER_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
    else:
        print(f"  âœ“ USER_ID: è¨­å®šæ¸ˆã¿ ({USER_ID})")

    print("\nâœ“ CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿")
    if not os.path.exists(CSV_FILE):
        print(f"  âœ— {CSV_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    else:
        posts = load_schedule()
        print(f"  âœ“ ãƒ•ã‚¡ã‚¤ãƒ«: {CSV_FILE}")
        print(f"  âœ“ æŠ•ç¨¿æ•°: {len(posts)}ä»¶")

    return True


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    global DRY_RUN, CSV_FILE

    parser = argparse.ArgumentParser(
        description='Threads API äºˆç´„æŠ•ç¨¿ + PDCAåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python threads_pdca.py                    # æŠ•ç¨¿ãƒã‚§ãƒƒã‚¯ï¼†å®Ÿè¡Œ
  python threads_pdca.py --dry-run          # æŠ•ç¨¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã«ã¯æŠ•ç¨¿ã—ãªã„ï¼‰
  python threads_pdca.py --csv test.csv     # åˆ¥ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
  python threads_pdca.py pdca                    # 3æ—¥é–“PDCAãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
  python threads_pdca.py suggest                 # æ¬¡ã®æŠ•ç¨¿ã‚’ææ¡ˆ
  python threads_pdca.py full-cycle              # ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«ï¼ˆåˆ†æâ†’ææ¡ˆï¼‰
  python threads_pdca.py analyze-competitors     # ç«¶åˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ†æ
        """
    )

    parser.add_argument(
        'command',
        nargs='?',
        choices=['pdca', 'suggest', 'full-cycle', 'analyze-competitors'],
        help='å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰ï¼ˆæŒ‡å®šã—ãªã„å ´åˆã¯æŠ•ç¨¿ãƒ¢ãƒ¼ãƒ‰ï¼‰'
    )

    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã«ã¯æŠ•ç¨¿ã›ãšã€å‹•ä½œç¢ºèªã®ã¿ï¼‰'
    )

    parser.add_argument(
        '--csv',
        type=str,
        help='ä½¿ç”¨ã™ã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: posts_schedule.csvï¼‰'
    )

    args = parser.parse_args()

    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
    if args.dry_run:
        DRY_RUN = True

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹è¨­å®š
    if args.csv:
        CSV_FILE = args.csv

    # ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
    if args.command == 'pdca':
        # PDCAãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        generate_3day_report()
    elif args.command == 'suggest':
        # æ¬¡ã®æŠ•ç¨¿ã‚’ææ¡ˆ
        suggest_next_posts()
    elif args.command == 'full-cycle':
        # ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«ï¼ˆåˆ†æâ†’ææ¡ˆï¼‰
        generate_3day_report()
        suggest_next_posts()
    elif args.command == 'analyze-competitors':
        # ç«¶åˆåˆ†æ
        analyze_competitors()
    else:
        # æŠ•ç¨¿ãƒ¢ãƒ¼ãƒ‰
        print("=" * 70)
        if DRY_RUN:
            print("Threads äºˆç´„æŠ•ç¨¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰)")
        else:
            print("Threads äºˆç´„æŠ•ç¨¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
        print("=" * 70)

        # è¨­å®šæ¤œè¨¼
        if not validate_config():
            print("\nâš ï¸  è¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)

        check_and_post()

        if DRY_RUN:
            print("\n" + "=" * 70)
            print("ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†")
            print("å®Ÿéš›ã«æŠ•ç¨¿ã™ã‚‹å ´åˆã¯ --dry-run ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å¤–ã—ã¦ãã ã•ã„")
            print("=" * 70)


if __name__ == '__main__':
    # ãƒ˜ãƒ«ãƒ—è¡¨ç¤ºã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if '-h' in sys.argv or '--help' in sys.argv:
        main()
        sys.exit(0)

    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
    is_dry_run = '--dry-run' in sys.argv

    # pdca, suggest, full-cycleã‚³ãƒãƒ³ãƒ‰ã¯èªè¨¼æƒ…å ±ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå®Ÿè¡Œæ™‚ã«ãƒã‚§ãƒƒã‚¯ã•ã‚Œã‚‹ï¼‰
    is_read_only_command = any(cmd in sys.argv for cmd in ['pdca', 'suggest', 'full-cycle'])

    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã¾ãŸã¯èª­ã¿å–ã‚Šå°‚ç”¨ã‚³ãƒãƒ³ãƒ‰ä»¥å¤–ã§ã¯èªè¨¼æƒ…å ±ãŒå¿…é ˆ
    if not is_dry_run and not is_read_only_command:
        if ACCESS_TOKEN == 'YOUR_ACCESS_TOKEN_HERE' or USER_ID == 'YOUR_USER_ID_HERE':
            print("\nâš ï¸  èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼")
            print("ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š")
            print("  export THREADS_ACCESS_TOKEN='your_token'")
            print("  export THREADS_USER_ID='your_user_id'")
            print("\nã¾ãŸã¯ã€å‹•ä½œç¢ºèªã ã‘ãªã‚‰ --dry-run ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š")
            print("  python threads_pdca.py --dry-run")
            sys.exit(1)

    main()
