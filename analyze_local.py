#!/usr/bin/env python3
"""
ãƒ­ãƒ¼ã‚«ãƒ«åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

posted_history.csv ã‹ã‚‰æŠ•ç¨¿IDã‚’å–å¾—ã—ã€
Threads API ã§è©³ç´°ãªåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™ã€‚

çµæœã¯ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã•ã‚Œã€ãƒªãƒã‚¸ãƒˆãƒªã«ã¯pushã•ã‚Œã¾ã›ã‚“ã€‚
"""

import csv
import requests
import json
import os
from datetime import datetime
from collections import defaultdict

# è¨­å®š
ACCESS_TOKEN = os.getenv('THREADS_ACCESS_TOKEN')
USER_ID = os.getenv('THREADS_USER_ID')
API_BASE_URL = 'https://graph.threads.net/v1.0'

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.gitignore ã«å«ã¾ã‚Œã¦ã„ã‚‹ï¼‰
ANALYSIS_FILE = 'analysis_results.json'
REPORT_FILE = 'analysis_report.md'


def get_user_posts():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŠ•ç¨¿ä¸€è¦§ã‚’å–å¾—"""
    url = f'{API_BASE_URL}/{USER_ID}/threads'
    params = {
        'fields': 'id,text,timestamp,media_type,permalink',
        'access_token': ACCESS_TOKEN,
        'limit': 100
    }

    print("ğŸ“¡ æŠ•ç¨¿ä¸€è¦§ã‚’å–å¾—ä¸­...")
    response = requests.get(url, params=params)
    response.raise_for_status()

    data = response.json()
    posts = data.get('data', [])
    print(f"âœ… {len(posts)} ä»¶ã®æŠ•ç¨¿ã‚’å–å¾—")

    return posts


def get_post_insights(post_id):
    """æŠ•ç¨¿ã®è©³ç´°ãªåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    url = f'{API_BASE_URL}/{post_id}/insights'
    params = {
        'metric': 'views,likes,replies,reposts,quotes,shares',
        'access_token': ACCESS_TOKEN
    }

    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        insights = {}
        for metric in data.get('data', []):
            insights[metric['name']] = metric['values'][0]['value']

        return insights
    except Exception as e:
        print(f"  âš ï¸  åˆ†æãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return {}


def load_posted_history():
    """posted_history.csv ã‹ã‚‰æŠ•ç¨¿æ¸ˆã¿IDã‚’å–å¾—"""
    posted_ids = []
    with open('posted_history.csv', 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            posted_ids.append({
                'csv_id': row['csv_id'],
                'posted_at': row['posted_at']
            })

    print(f"ğŸ“‹ æŠ•ç¨¿å±¥æ­´: {len(posted_ids)} ä»¶")
    return posted_ids


def load_posts_schedule():
    """posts_schedule.csv ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—"""
    categories = {}
    if os.path.exists('posts_schedule.csv'):
        with open('posts_schedule.csv', 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                categories[row['id']] = row.get('category', 'æœªåˆ†é¡')

    return categories


def analyze_posts():
    """æŠ•ç¨¿ã‚’åˆ†æ"""
    print("=" * 70)
    print("Threads æŠ•ç¨¿åˆ†æï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œï¼‰")
    print("=" * 70)

    # 1. æŠ•ç¨¿å±¥æ­´ã‚’èª­ã¿è¾¼ã¿
    posted_history = load_posted_history()

    # 2. ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    categories = load_posts_schedule()

    # 3. Threads API ã‹ã‚‰æŠ•ç¨¿ä¸€è¦§ã‚’å–å¾—
    posts = get_user_posts()

    # 4. å„æŠ•ç¨¿ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    print("\nğŸ“Š å„æŠ•ç¨¿ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\n")

    analysis_results = []

    for i, post in enumerate(posts[:20]):  # æœ€æ–°20ä»¶ã‚’åˆ†æ
        post_id = post['id']
        text_preview = post.get('text', '')[:50]
        timestamp = post.get('timestamp', '')

        print(f"[{i+1}/20] {text_preview}...")

        # åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        insights = get_post_insights(post_id)

        result = {
            'post_id': post_id,
            'text': post.get('text', ''),
            'timestamp': timestamp,
            'permalink': post.get('permalink', ''),
            'views': insights.get('views', 0),
            'likes': insights.get('likes', 0),
            'replies': insights.get('replies', 0),
            'reposts': insights.get('reposts', 0),
            'quotes': insights.get('quotes', 0),
            'shares': insights.get('shares', 0),
            'engagement_rate': 0
        }

        # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã‚’è¨ˆç®—
        if result['views'] > 0:
            total_engagement = result['likes'] + result['replies'] + result['reposts']
            result['engagement_rate'] = (total_engagement / result['views']) * 100

        analysis_results.append(result)

    # 5. çµæœã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
    print(f"\nğŸ’¾ åˆ†æçµæœã‚’ä¿å­˜ä¸­: {ANALYSIS_FILE}")

    with open(ANALYSIS_FILE, 'w', encoding='utf-8') as f:
        json.dump({
            'analyzed_at': datetime.now().isoformat(),
            'total_posts': len(analysis_results),
            'results': analysis_results
        }, f, ensure_ascii=False, indent=2)

    print(f"âœ… ä¿å­˜å®Œäº†: {ANALYSIS_FILE}")

    # 6. ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
    generate_report(analysis_results)

    return analysis_results


def generate_report(results):
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    print(f"\nğŸ“ ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­: {REPORT_FILE}")

    # ã‚½ãƒ¼ãƒˆ: ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡é †
    sorted_by_engagement = sorted(results, key=lambda x: x['engagement_rate'], reverse=True)

    # ã‚½ãƒ¼ãƒˆ: ã„ã„ã­æ•°é †
    sorted_by_likes = sorted(results, key=lambda x: x['likes'], reverse=True)

    # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    report = []
    report.append("# ğŸ“Š Threads æŠ•ç¨¿åˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n")
    report.append(f"**åˆ†ææ—¥æ™‚:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    report.append(f"**åˆ†æä»¶æ•°:** {len(results)} ä»¶\n")
    report.append("\n---\n")

    # ã‚µãƒãƒªãƒ¼
    total_views = sum(r['views'] for r in results)
    total_likes = sum(r['likes'] for r in results)
    total_replies = sum(r['replies'] for r in results)
    avg_engagement = sum(r['engagement_rate'] for r in results) / len(results) if results else 0

    report.append("## ğŸ“ˆ ã‚µãƒãƒªãƒ¼\n")
    report.append(f"- **åˆè¨ˆãƒ“ãƒ¥ãƒ¼æ•°:** {total_views:,}\n")
    report.append(f"- **åˆè¨ˆã„ã„ã­æ•°:** {total_likes:,}\n")
    report.append(f"- **åˆè¨ˆãƒªãƒ—ãƒ©ã‚¤æ•°:** {total_replies:,}\n")
    report.append(f"- **å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡:** {avg_engagement:.2f}%\n")
    report.append("\n---\n")

    # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ TOP 5
    report.append("## ğŸ”¥ ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ TOP 5\n")
    for i, post in enumerate(sorted_by_engagement[:5], 1):
        report.append(f"### {i}. ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡: {post['engagement_rate']:.2f}%\n")
        report.append(f"- **æŠ•ç¨¿å†…å®¹:** {post['text'][:100]}...\n")
        report.append(f"- **ãƒ“ãƒ¥ãƒ¼æ•°:** {post['views']:,}\n")
        report.append(f"- **ã„ã„ã­æ•°:** {post['likes']:,}\n")
        report.append(f"- **ãƒªãƒ—ãƒ©ã‚¤æ•°:** {post['replies']:,}\n")
        report.append(f"- **ãƒªãƒ³ã‚¯:** {post['permalink']}\n")
        report.append("\n")

    report.append("---\n")

    # ã„ã„ã­æ•° TOP 5
    report.append("## â¤ï¸ ã„ã„ã­æ•° TOP 5\n")
    for i, post in enumerate(sorted_by_likes[:5], 1):
        report.append(f"### {i}. ã„ã„ã­æ•°: {post['likes']:,}\n")
        report.append(f"- **æŠ•ç¨¿å†…å®¹:** {post['text'][:100]}...\n")
        report.append(f"- **ãƒ“ãƒ¥ãƒ¼æ•°:** {post['views']:,}\n")
        report.append(f"- **ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡:** {post['engagement_rate']:.2f}%\n")
        report.append(f"- **ãƒªãƒ³ã‚¯:** {post['permalink']}\n")
        report.append("\n")

    report.append("---\n")

    # æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ
    report.append("## ğŸ’¡ æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ\n")
    report.append("åˆ†æçµæœã‹ã‚‰ä»¥ä¸‹ã®ãƒã‚¤ãƒ³ãƒˆã‚’å‚è€ƒã«ã€æ–°ã—ã„æŠ•ç¨¿ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š\n\n")

    best_post = sorted_by_engagement[0] if sorted_by_engagement else None
    if best_post:
        report.append(f"1. **æœ€ã‚‚åå¿œãŒè‰¯ã‹ã£ãŸæŠ•ç¨¿ã®ç‰¹å¾´ã‚’åˆ†æ**\n")
        report.append(f"   - ãƒ†ãƒ¼ãƒã€ãƒˆãƒ¼ãƒ³ã€æ–‡ç« æ§‹æˆã‚’ç¢ºèª\n")
        report.append(f"   - ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡: {best_post['engagement_rate']:.2f}%\n\n")

    avg_views = total_views / len(results) if results else 0
    report.append(f"2. **å¹³å‡ãƒ“ãƒ¥ãƒ¼æ•°ã‚’ä¸Šå›ã‚‹æŠ•ç¨¿ã‚’å¢—ã‚„ã™**\n")
    report.append(f"   - ç¾åœ¨ã®å¹³å‡: {avg_views:,.0f} ãƒ“ãƒ¥ãƒ¼\n")
    report.append(f"   - ç›®æ¨™: {avg_views * 1.5:,.0f} ãƒ“ãƒ¥ãƒ¼ä»¥ä¸Š\n\n")

    report.append(f"3. **ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ã‚’å‘ä¸Šã•ã›ã‚‹**\n")
    report.append(f"   - ç¾åœ¨ã®å¹³å‡: {avg_engagement:.2f}%\n")
    report.append(f"   - ç›®æ¨™: {avg_engagement * 1.2:.2f}% ä»¥ä¸Š\n\n")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(REPORT_FILE, 'w', encoding='utf-8') as f:
        f.writelines(report)

    print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {REPORT_FILE}")
    print("\nğŸ“– ãƒ¬ãƒãƒ¼ãƒˆã‚’é–‹ã:")
    print(f"  open {REPORT_FILE}")


def main():
    if not ACCESS_TOKEN or not USER_ID:
        print("âŒ ã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("\n.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„:")
        print("  export $(cat .env | xargs)")
        print("  python3 analyze_local.py")
        return

    try:
        results = analyze_posts()

        print("\n" + "=" * 70)
        print("âœ… åˆ†æå®Œäº†ï¼")
        print("=" * 70)
        print(f"\nçµæœ:")
        print(f"  - {ANALYSIS_FILE} (JSONå½¢å¼)")
        print(f"  - {REPORT_FILE} (ãƒ¬ãƒãƒ¼ãƒˆ)")
        print("\nã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã®ã¿ã«ä¿å­˜ã•ã‚Œã€")
        print("ãƒªãƒã‚¸ãƒˆãƒªã«ã¯pushã•ã‚Œã¾ã›ã‚“ï¼ˆ.gitignoreè¨­å®šæ¸ˆã¿ï¼‰")
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("  1. ãƒ¬ãƒãƒ¼ãƒˆã‚’ç¢ºèª")
        print("  2. åå¿œã®è‰¯ã‹ã£ãŸæŠ•ç¨¿ã‚’å‚è€ƒã«æ–°ã—ã„æŠ•ç¨¿ã‚’ä½œæˆ")
        print("  3. posts_schedule.csv ã«è¿½åŠ ")
        print("  4. git push")

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
