#!/usr/bin/env python3
"""
Threads API äºˆç´„æŠ•ç¨¿ + PDCAåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ (SQLiteç‰ˆ)
3æ—¥ã‚µã‚¤ã‚¯ãƒ«ã§PDCAã‚’å›ã™ã“ã¨ã«ç‰¹åŒ–
"""

import sqlite3
import csv
import time
import requests
import json
import argparse
import os
import sys
import re
from datetime import datetime, timedelta
from collections import defaultdict


# ============================================
# è¨­å®šé …ç›®
# ============================================
ACCESS_TOKEN = os.getenv('THREADS_ACCESS_TOKEN', 'YOUR_ACCESS_TOKEN_HERE')
USER_ID = os.getenv('THREADS_USER_ID', 'YOUR_USER_ID_HERE')

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
DB_FILE = os.getenv('DB_FILE', 'threads.db')
PDCA_REPORT_FILE = 'pdca_report.md'
COMPETITOR_REPORT_FILE = 'competitor_report.md'

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
MIN_INTERVAL_SECONDS = 3600  # 1æ™‚é–“

# ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰
DRY_RUN = False

# ============================================
# Threads API
# ============================================
API_BASE_URL = 'https://graph.threads.net/v1.0'


def get_db_connection():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—"""
    conn = sqlite3.connect(DB_FILE)
    conn.row_factory = sqlite3.Row  # è¾æ›¸å½¢å¼ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«
    return conn


def create_threads_post(text):
    """Threads APIã§æŠ•ç¨¿ã‚’ä½œæˆ"""
    global DRY_RUN

    if DRY_RUN:
        print(f"  â†’ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] æŠ•ç¨¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆä¸­...")
        print(f"  â†’ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] å®Ÿéš›ã«ã¯æŠ•ç¨¿ã•ã‚Œã¾ã›ã‚“")
        time.sleep(0.5)
        fake_post_id = f"dry_run_{int(time.time())}"
        print(f"  âœ“ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] æŠ•ç¨¿æˆåŠŸï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰ï¼ (ID: {fake_post_id})")
        return fake_post_id

    try:
        create_url = f'{API_BASE_URL}/{USER_ID}/threads'
        create_params = {
            'media_type': 'TEXT',
            'text': text,
            'access_token': ACCESS_TOKEN
        }

        print(f"  â†’ ã‚³ãƒ³ãƒ†ãƒŠä½œæˆä¸­...")
        create_response = requests.post(create_url, params=create_params)
        create_response.raise_for_status()
        container_id = create_response.json().get('id')

        if not container_id:
            print(f"  âœ— ã‚³ãƒ³ãƒ†ãƒŠIDã®å–å¾—ã«å¤±æ•—")
            return None

        publish_url = f'{API_BASE_URL}/{USER_ID}/threads_publish'
        publish_params = {
            'creation_id': container_id,
            'access_token': ACCESS_TOKEN
        }

        print(f"  â†’ æŠ•ç¨¿å…¬é–‹ä¸­...")
        publish_response = requests.post(publish_url, params=publish_params)
        publish_response.raise_for_status()

        post_id = publish_response.json().get('id')
        if post_id:
            print(f"  âœ“ æŠ•ç¨¿æˆåŠŸï¼ (ID: {post_id})")
            return post_id
        else:
            print(f"  âœ— æŠ•ç¨¿IDã®å–å¾—ã«å¤±æ•—")
            return None

    except requests.exceptions.RequestException as e:
        print(f"  âœ— API ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def get_post_insights(threads_post_id):
    """æŠ•ç¨¿ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        url = f'{API_BASE_URL}/{threads_post_id}'
        params = {
            'fields': 'id,text,timestamp,permalink',
            'access_token': ACCESS_TOKEN
        }

        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        insights_url = f'{API_BASE_URL}/{threads_post_id}/insights'
        insights_params = {
            'metric': 'views,likes,replies,reposts,quotes',
            'access_token': ACCESS_TOKEN
        }

        insights_response = requests.get(insights_url, params=insights_params)

        insights_data = {}
        if insights_response.status_code == 200:
            insights = insights_response.json().get('data', [])
            for metric in insights:
                insights_data[metric['name']] = metric.get('values', [{}])[0].get('value', 0)

        result = {
            'threads_post_id': threads_post_id,
            'text': data.get('text', ''),
            'timestamp': data.get('timestamp', ''),
            'permalink': data.get('permalink', ''),
            'views': insights_data.get('views', 0),
            'likes': insights_data.get('likes', 0),
            'replies': insights_data.get('replies', 0),
            'reposts': insights_data.get('reposts', 0),
            'quotes': insights_data.get('quotes', 0),
        }

        total_engagement = (
            result['likes'] +
            result['replies'] +
            result['reposts'] +
            result['quotes']
        )
        result['engagement'] = total_engagement
        result['engagement_rate'] = (
            (total_engagement / result['views'] * 100)
            if result['views'] > 0 else 0
        )

        return result

    except requests.exceptions.RequestException as e:
        print(f"  âœ— åˆ†æãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


# ============================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ
# ============================================

def get_pending_posts():
    """æœªæŠ•ç¨¿ã§æŠ•ç¨¿æ™‚åˆ»ã‚’éããŸã‚‚ã®ã‚’å–å¾—"""
    conn = get_db_connection()
    cursor = conn.cursor()

    # æ—¥æœ¬æ™‚é–“ï¼ˆJST = UTC+9ï¼‰ã§ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
    from datetime import timezone, timedelta
    jst = timezone(timedelta(hours=9))
    current_time = datetime.now(jst).strftime('%Y-%m-%d %H:%M:%S')

    cursor.execute("""
        SELECT id, csv_id, scheduled_at, text, category
        FROM posts
        WHERE status = 'pending'
          AND scheduled_at <= ?
        ORDER BY scheduled_at
    """, (current_time,))

    posts = cursor.fetchall()
    conn.close()

    return [dict(row) for row in posts]


def mark_as_posted(post_id, threads_post_id):
    """æŠ•ç¨¿ã‚’æŠ•ç¨¿æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
    conn = get_db_connection()
    cursor = conn.cursor()

    cursor.execute("""
        UPDATE posts
        SET status = 'posted',
            threads_post_id = ?,
            posted_at = ?,
            updated_at = CURRENT_TIMESTAMP
        WHERE id = ?
    """, (threads_post_id, datetime.now(), post_id))

    conn.commit()
    conn.close()


def mark_as_failed(post_id, error_message):
    """æŠ•ç¨¿ã‚’å¤±æ•—ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
    conn = get_db_connection()
    cursor = conn.cursor()

    cursor.execute("""
        UPDATE posts
        SET status = 'failed',
            error_message = ?,
            updated_at = CURRENT_TIMESTAMP
        WHERE id = ?
    """, (error_message, post_id))

    conn.commit()
    conn.close()


def save_analytics(post_id, analytics_data):
    """åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    conn = get_db_connection()
    cursor = conn.cursor()

    cursor.execute("""
        INSERT INTO analytics (
            post_id, threads_post_id,
            views, likes, replies, reposts, quotes,
            engagement, engagement_rate,
            permalink, fetched_at
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        post_id,
        analytics_data['threads_post_id'],
        analytics_data['views'],
        analytics_data['likes'],
        analytics_data['replies'],
        analytics_data['reposts'],
        analytics_data['quotes'],
        analytics_data['engagement'],
        analytics_data['engagement_rate'],
        analytics_data['permalink'],
        datetime.now()
    ))

    conn.commit()
    conn.close()


def get_recent_posts(days=3):
    """éå»Næ—¥é–“ã®æŠ•ç¨¿æ¸ˆã¿æŠ•ç¨¿ã‚’å–å¾—"""
    conn = get_db_connection()
    cursor = conn.cursor()

    cutoff_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d %H:%M:%S')

    cursor.execute("""
        SELECT id, csv_id, text, posted_at, threads_post_id, category, scheduled_at
        FROM posts
        WHERE status = 'posted'
          AND posted_at >= ?
        ORDER BY posted_at DESC
    """, (cutoff_date,))

    posts = cursor.fetchall()
    conn.close()

    return [dict(row) for row in posts]


# ============================================
# æŠ•ç¨¿ç®¡ç†ã‚³ãƒãƒ³ãƒ‰
# ============================================

def add_post(scheduled_at, text, category=None):
    """æ–°ã—ã„æŠ•ç¨¿ã‚’è¿½åŠ """
    conn = get_db_connection()
    cursor = conn.cursor()

    # æœ€æ–°ã®csv_idã‚’å–å¾—
    cursor.execute("SELECT MAX(CAST(csv_id AS INTEGER)) FROM posts WHERE csv_id GLOB '[0-9]*'")
    result = cursor.fetchone()
    max_id = result[0] if result[0] else 0
    new_csv_id = str(max_id + 1)

    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
    char_count = len(text)
    has_emoji = any(ord(c) > 127 for c in text)

    # ã‚«ãƒ†ã‚´ãƒªè‡ªå‹•æ¤œå‡º
    if not category:
        category = detect_category(text)

    cursor.execute("""
        INSERT INTO posts (
            csv_id, scheduled_at, text, status,
            category, char_count, has_emoji
        ) VALUES (?, ?, ?, 'pending', ?, ?, ?)
    """, (new_csv_id, scheduled_at, text, category, char_count, has_emoji))

    conn.commit()
    post_id = cursor.lastrowid
    conn.close()

    print(f"âœ… æŠ•ç¨¿ã‚’è¿½åŠ ã—ã¾ã—ãŸ (ID: {new_csv_id})")
    print(f"   æ—¥æ™‚: {scheduled_at}")
    print(f"   ã‚«ãƒ†ã‚´ãƒª: {category}")
    print(f"   æ–‡å­—æ•°: {char_count}æ–‡å­—")

    return post_id


def detect_category(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š"""
    keywords = {
        'æ‹æ„›': ['å½¼æ°', 'å½¼å¥³', 'æ‹æ„›', 'ãƒãƒƒãƒãƒ³ã‚°', 'å‡ºä¼šã„', 'å¥½ã', 'ãƒ‡ãƒ¼ãƒˆ', 'çµå©š'],
        'ä»•äº‹': ['è»¢è·', 'ä»•äº‹', 'è·å ´', 'ä¼šç¤¾', 'ä¸Šå¸', 'çµ¦æ–™', 'æ®‹æ¥­', 'ã‚­ãƒ£ãƒªã‚¢'],
        'ãŠé‡‘': ['è²¯é‡‘', 'ãŠé‡‘', 'ç¯€ç´„', 'NISA', 'æŠ•è³‡', 'ã‚µãƒ–ã‚¹ã‚¯', 'ã‚¯ãƒ¬ã‚«', 'æµªè²»'],
        'ãƒ¡ãƒ³ã‚¿ãƒ«': ['HSP', 'ç¹Šç´°', 'è‡ªå·±è‚¯å®šæ„Ÿ', 'ã‚¹ãƒˆãƒ¬ã‚¹', 'ç–²ã‚Œ', 'ä¸å®‰', 'ãƒ¡ãƒ³ã‚¿ãƒ«'],
        'å ã„': ['å ã„', 'æœˆæ˜Ÿåº§', 'æ•°ç§˜', 'ã‚¿ãƒ­ãƒƒãƒˆ', 'ãƒ›ãƒ­ã‚¹ã‚³ãƒ¼ãƒ—', 'ã‚¹ãƒ”ãƒªãƒãƒ¥ã‚¢ãƒ«']
    }

    for category, words in keywords.items():
        if any(word in text for word in words):
            return category

    return 'ãã®ä»–'


def list_posts(status=None, limit=20, today=False, tomorrow=False):
    """æŠ•ç¨¿ä¸€è¦§ã‚’è¡¨ç¤º"""
    conn = get_db_connection()
    cursor = conn.cursor()

    query = "SELECT id, csv_id, scheduled_at, substr(text, 1, 50) as preview, status, category FROM posts"
    params = []

    conditions = []
    if status:
        conditions.append("status = ?")
        params.append(status)

    if today:
        conditions.append("DATE(scheduled_at) = DATE('now', 'localtime')")
    elif tomorrow:
        conditions.append("DATE(scheduled_at) = DATE('now', 'localtime', '+1 day')")

    if conditions:
        query += " WHERE " + " AND ".join(conditions)

    query += f" ORDER BY scheduled_at DESC LIMIT {limit}"

    cursor.execute(query, params)
    posts = cursor.fetchall()
    conn.close()

    if not posts:
        print("æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    print(f"\næŠ•ç¨¿ä¸€è¦§ ({len(posts)}ä»¶):")
    print("-" * 100)

    for post in posts:
        post_dict = dict(post)
        print(f"[{post_dict['csv_id']}] {post_dict['scheduled_at']} | {post_dict['status']:8s} | {post_dict['category']:8s}")
        print(f"      {post_dict['preview']}...")
        print()


def import_from_csv(csv_file):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ•ç¨¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
    if not os.path.exists(csv_file):
        print(f"âœ— ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
        return

    conn = get_db_connection()
    cursor = conn.cursor()

    imported = 0
    skipped = 0

    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)

        for row in reader:
            csv_id = row.get('id', '').strip()
            datetime_str = row.get('datetime', '').strip()
            text = row.get('text', '').strip()
            category = row.get('category', '').strip() or None

            if not csv_id or not datetime_str or not text:
                skipped += 1
                continue

            try:
                scheduled_at = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M')
                char_count = len(text)
                has_emoji = any(ord(c) > 127 for c in text)

                if not category:
                    category = detect_category(text)

                cursor.execute("""
                    INSERT INTO posts (
                        csv_id, scheduled_at, text, status,
                        category, char_count, has_emoji
                    ) VALUES (?, ?, ?, 'pending', ?, ?, ?)
                """, (csv_id, scheduled_at, text, category, char_count, has_emoji))

                imported += 1

            except sqlite3.IntegrityError:
                skipped += 1
            except Exception as e:
                print(f"âœ— ã‚¨ãƒ©ãƒ¼ (ID: {csv_id}): {e}")
                skipped += 1

    conn.commit()
    conn.close()

    print(f"âœ… ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†:")
    print(f"   æˆåŠŸ: {imported}ä»¶")
    print(f"   ã‚¹ã‚­ãƒƒãƒ—: {skipped}ä»¶")


def export_to_csv(output_file, status=None):
    """æŠ•ç¨¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    conn = get_db_connection()
    cursor = conn.cursor()

    query = "SELECT csv_id, scheduled_at, text, status, category FROM posts"
    params = []

    if status:
        query += " WHERE status = ?"
        params.append(status)

    query += " ORDER BY scheduled_at"

    cursor.execute(query, params)
    posts = cursor.fetchall()
    conn.close()

    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['id', 'datetime', 'text', 'status', 'category'])

        for post in posts:
            post_dict = dict(post)
            dt = datetime.fromisoformat(post_dict['scheduled_at']).strftime('%Y-%m-%d %H:%M')
            writer.writerow([
                post_dict['csv_id'],
                dt,
                post_dict['text'],
                post_dict['status'],
                post_dict['category']
            ])

    print(f"âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {len(posts)}ä»¶ â†’ {output_file}")


# ============================================
# æŠ•ç¨¿å‡¦ç†
# ============================================

def check_and_post():
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æŠ•ç¨¿"""
    global DRY_RUN
    from datetime import timezone, timedelta

    # ç¾åœ¨æ™‚åˆ»ã‚’è¡¨ç¤º
    jst = timezone(timedelta(hours=9))
    current_time_jst = datetime.now(jst).strftime('%Y-%m-%d %H:%M:%S')
    print(f"\nğŸ“… ç¾åœ¨æ™‚åˆ»ï¼ˆJSTï¼‰: {current_time_jst}")

    posts = get_pending_posts()

    if not posts:
        print("æŠ•ç¨¿å¾…ã¡ã®é …ç›®ã¯ã‚ã‚Šã¾ã›ã‚“")
        return

    print(f"\nâœ… æŠ•ç¨¿å¯¾è±¡: {len(posts)}ä»¶")
    print(f"æ™‚åˆ»ç¯„å›²: {posts[0]['scheduled_at']} ã€œ {posts[-1]['scheduled_at']}")

    if DRY_RUN:
        print("\n[ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] ä»¥ä¸‹ã®æŠ•ç¨¿ãŒå®Ÿè¡Œã•ã‚Œã¾ã™:\n")

    for i, post in enumerate(posts):
        post_id = post['id']
        csv_id = post['csv_id']
        text = post['text']
        scheduled_at = post['scheduled_at']
        category = post.get('category', 'æœªåˆ†é¡')

        print(f"\n[{i+1}/{len(posts)}] æŠ•ç¨¿ID: {csv_id} | {scheduled_at} | [{category}]")
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text[:80]}{'...' if len(text) > 80 else ''}")

        if DRY_RUN:
            print(f"  â†’ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] å®Ÿéš›ã«ã¯æŠ•ç¨¿ã•ã‚Œã¾ã›ã‚“")
            continue

        threads_post_id = create_threads_post(text)

        if threads_post_id:
            mark_as_posted(post_id, threads_post_id)
        else:
            mark_as_failed(post_id, "APIæŠ•ç¨¿ã‚¨ãƒ©ãƒ¼")

        if i < len(posts) - 1:
            wait_time = MIN_INTERVAL_SECONDS
            print(f"  â†’ æ¬¡ã®æŠ•ç¨¿ã¾ã§ {wait_time}ç§’å¾…æ©Ÿ...")
            time.sleep(wait_time)


# ============================================
# PDCAåˆ†æ
# ============================================

def generate_pdca_report(days=3):
    """PDCAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    print("\n" + "="*70)
    print(f"ğŸ“Š éå»{days}æ—¥é–“ã®PDCAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    print("="*70)

    recent_posts = get_recent_posts(days)

    if not recent_posts:
        print(f"âš ï¸  éå»{days}æ—¥é–“ã®æŠ•ç¨¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    print(f"\néå»{days}æ—¥é–“ã®æŠ•ç¨¿æ•°: {len(recent_posts)}ä»¶")
    print("åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\n")

    # å„æŠ•ç¨¿ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    analytics_results = []
    for i, post in enumerate(recent_posts, 1):
        print(f"[{i}/{len(recent_posts)}] æŠ•ç¨¿ {post['csv_id']} ã‚’åˆ†æä¸­...")
        analytics = get_post_insights(post['threads_post_id'])
        if analytics:
            analytics['posted_at'] = datetime.fromisoformat(post['posted_at'])
            analytics['csv_id'] = post['csv_id']
            analytics['category'] = post['category']
            analytics_results.append(analytics)

            # DB ã«ä¿å­˜
            save_analytics(post['id'], analytics)
            print(f"  âœ“ å®Œäº†")
        time.sleep(2)

    if not analytics_results:
        print("âš ï¸  åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = generate_pdca_markdown(analytics_results, days)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(PDCA_REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nâœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ {PDCA_REPORT_FILE} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print("\n" + report)


def generate_pdca_markdown(analytics_results, days):
    """PDCAãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownå½¢å¼ã§ç”Ÿæˆ"""
    total_posts = len(analytics_results)
    total_views = sum(a['views'] for a in analytics_results)
    total_engagement = sum(a['engagement'] for a in analytics_results)
    avg_engagement_rate = sum(a['engagement_rate'] for a in analytics_results) / total_posts

    # æ™‚é–“å¸¯åˆ¥åˆ†æ
    hourly_performance = defaultdict(lambda: {'count': 0, 'engagement_rate': 0})
    for a in analytics_results:
        hour = a['posted_at'].hour
        hourly_performance[hour]['count'] += 1
        hourly_performance[hour]['engagement_rate'] += a['engagement_rate']

    for hour in hourly_performance:
        hourly_performance[hour]['avg_engagement_rate'] = (
            hourly_performance[hour]['engagement_rate'] /
            hourly_performance[hour]['count']
        )

    # ãƒ™ã‚¹ãƒˆæ™‚é–“å¸¯
    best_hours = sorted(
        hourly_performance.items(),
        key=lambda x: x[1]['avg_engagement_rate'],
        reverse=True
    )[:3]

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    category_performance = defaultdict(lambda: {'count': 0, 'engagement_rate': 0})
    for a in analytics_results:
        cat = a.get('category', 'ãã®ä»–')
        category_performance[cat]['count'] += 1
        category_performance[cat]['engagement_rate'] += a['engagement_rate']

    for cat in category_performance:
        category_performance[cat]['avg_engagement_rate'] = (
            category_performance[cat]['engagement_rate'] /
            category_performance[cat]['count']
        )

    # ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿
    top_posts = sorted(analytics_results, key=lambda x: x['engagement_rate'], reverse=True)[:3]
    worst_posts = sorted(analytics_results, key=lambda x: x['engagement_rate'])[:3]

    # æ–‡å­—æ•°åˆ†æ
    char_counts = [len(a['text']) for a in analytics_results]
    avg_char_count = sum(char_counts) / len(char_counts) if char_counts else 0

    # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    now = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')

    report = f"""# ğŸ“Š Threads PDCA ãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {now}
**åˆ†ææœŸé–“**: éå»{days}æ—¥é–“
**æŠ•ç¨¿æ•°**: {total_posts}ä»¶

---

## ğŸ“ˆ ã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| **ç·è¡¨ç¤ºå›æ•°** | {total_views:,} |
| **ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ** | {total_engagement:,} |
| **å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡** | {avg_engagement_rate:.2f}% |
| **å¹³å‡æ–‡å­—æ•°** | {avg_char_count:.0f}æ–‡å­— |

---

## ğŸ† ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿

"""

    for i, post in enumerate(top_posts, 1):
        report += f"""### {i}ä½: {post['engagement_rate']:.2f}% [{post.get('category', '')}]

- **ãƒ†ã‚­ã‚¹ãƒˆ**: {post['text'][:80]}...
- **è¡¨ç¤ºå›æ•°**: {post['views']:,}
- **ã„ã„ã­**: {post['likes']:,} | **è¿”ä¿¡**: {post['replies']:,} | **ãƒªãƒã‚¹ãƒˆ**: {post['reposts']:,}
- **æŠ•ç¨¿æ™‚åˆ»**: {post['posted_at'].strftime('%m/%d %H:%M')}

"""

    report += f"""---

## ğŸ¯ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

"""

    for cat, data in sorted(category_performance.items(), key=lambda x: x[1]['avg_engagement_rate'], reverse=True):
        report += f"- **{cat}**: {data['avg_engagement_rate']:.2f}% (æŠ•ç¨¿æ•°: {data['count']}ä»¶)\n"

    report += f"""

---

## â° ãƒ™ã‚¹ãƒˆæŠ•ç¨¿æ™‚é–“å¸¯

"""

    for i, (hour, data) in enumerate(best_hours, 1):
        report += f"{i}. **{hour:02d}æ™‚å°**: ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ {data['avg_engagement_rate']:.2f}% (æŠ•ç¨¿æ•°: {data['count']}ä»¶)\n"

    report += f"""

---

## ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ

### æ–‡å­—æ•°
- **å¹³å‡æ–‡å­—æ•°**: {avg_char_count:.0f}æ–‡å­—
- **æœ€çŸ­**: {min(char_counts)}æ–‡å­— | **æœ€é•·**: {max(char_counts)}æ–‡å­—

---

## âš ï¸ æ”¹å–„ãŒå¿…è¦ãªæŠ•ç¨¿

"""

    for i, post in enumerate(worst_posts, 1):
        report += f"""{i}. **{post['engagement_rate']:.2f}%** [{post.get('category', '')}] - {post['text'][:60]}...
"""

    report += f"""

---

## ğŸ’¡ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ (Plan)

### âœ… ç¶šã‘ã‚‹ã“ã¨ (Keep)

"""

    if best_hours:
        best_hour = best_hours[0][0]
        report += f"1. **{best_hour:02d}æ™‚å°ã®æŠ•ç¨¿ã‚’å¢—ã‚„ã™** - æœ€ã‚‚åå¿œãŒè‰¯ã„æ™‚é–“å¸¯ã§ã™\n"

    # ãƒˆãƒƒãƒ—ã‚«ãƒ†ã‚´ãƒª
    top_category = max(category_performance.items(), key=lambda x: x[1]['avg_engagement_rate'])
    report += f"2. **{top_category[0]}ã‚«ãƒ†ã‚´ãƒªãŒå¥½èª¿** - ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ {top_category[1]['avg_engagement_rate']:.2f}%\n"

    if top_posts:
        top_post = top_posts[0]
        if len(top_post['text']) < 200:
            report += f"3. **çŸ­ã‚ã®æŠ•ç¨¿ãŒå¥½èª¿** - {len(top_post['text'])}æ–‡å­—ç¨‹åº¦ãŒåŠ¹æœçš„\n"
        else:
            report += f"3. **é•·æ–‡æŠ•ç¨¿ãŒå¥½èª¿** - {len(top_post['text'])}æ–‡å­—ç¨‹åº¦ã®è©³ç´°ãªå†…å®¹ãŒåŠ¹æœçš„\n"

    report += f"""

### ğŸ”„ æ”¹å–„ã™ã‚‹ã“ã¨ (Improve)

"""

    # ãƒ¯ãƒ¼ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒª
    worst_category = min(category_performance.items(), key=lambda x: x[1]['avg_engagement_rate'])
    report += f"1. **{worst_category[0]}ã‚«ãƒ†ã‚´ãƒªã®æ”¹å–„** - ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãŒä½ã„ ({worst_category[1]['avg_engagement_rate']:.2f}%)\n"
    report += f"2. **ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã®åˆ†æ** - ãªãœåå¿œãŒæ‚ªã‹ã£ãŸã®ã‹æŒ¯ã‚Šè¿”ã‚‹\n"

    report += f"""

### ğŸ†• è©¦ã™ã“ã¨ (Try)

1. **æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’è©¦ã™** - è³ªå•å½¢å¼ã€æŠ•ç¥¨ã€ãƒªã‚¹ãƒˆå½¢å¼ãªã©
2. **ç•°ãªã‚‹æ™‚é–“å¸¯ã«ãƒ†ã‚¹ãƒˆæŠ•ç¨¿** - ã¾ã è©¦ã—ã¦ã„ãªã„æ™‚é–“å¸¯ã‚’æ¢ã‚‹
3. **åå¿œãŒè‰¯ã„ã‚«ãƒ†ã‚´ãƒªã‚’å¢—ã‚„ã™** - {top_category[0]}ã®æŠ•ç¨¿ã‚’å¢—ã‚„ã™

---

**æ¬¡å›ãƒ¬ãƒãƒ¼ãƒˆ**: {days}æ—¥å¾Œ

"""

    return report


# ============================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================

def validate_config():
    """è¨­å®šã®æ¤œè¨¼"""
    global DRY_RUN

    print("\nâœ“ ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª")

    has_token = ACCESS_TOKEN != 'YOUR_ACCESS_TOKEN_HERE'
    has_user_id = USER_ID != 'YOUR_USER_ID_HERE'

    if not has_token:
        if DRY_RUN:
            print("  âš  THREADS_ACCESS_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        else:
            print("  âœ— THREADS_ACCESS_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
    else:
        masked_token = ACCESS_TOKEN[:8] + "..." if len(ACCESS_TOKEN) > 8 else "***"
        print(f"  âœ“ ACCESS_TOKEN: è¨­å®šæ¸ˆã¿ ({masked_token})")

    if not has_user_id:
        if DRY_RUN:
            print("  âš  THREADS_USER_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        else:
            print("  âœ— THREADS_USER_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
    else:
        print(f"  âœ“ USER_ID: è¨­å®šæ¸ˆã¿ ({USER_ID})")

    print("\nâœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç¢ºèª")
    if not os.path.exists(DB_FILE):
        print(f"  âœ— {DB_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"  â†’ python migrate_to_sqlite.py full ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return False
    else:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM posts")
        count = cursor.fetchone()[0]
        conn.close()
        print(f"  âœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {DB_FILE}")
        print(f"  âœ“ æŠ•ç¨¿æ•°: {count}ä»¶")

    return True


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    global DRY_RUN, DB_FILE

    parser = argparse.ArgumentParser(
        description='Threads API äºˆç´„æŠ•ç¨¿ + PDCAåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ (SQLiteç‰ˆ)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # æŠ•ç¨¿å®Ÿè¡Œ
  python threads_sqlite.py post                # æŠ•ç¨¿ãƒã‚§ãƒƒã‚¯ï¼†å®Ÿè¡Œ
  python threads_sqlite.py post --dry-run      # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³

  # æŠ•ç¨¿ç®¡ç†
  python threads_sqlite.py list                # æŠ•ç¨¿ä¸€è¦§
  python threads_sqlite.py list --status pending --limit 10
  python threads_sqlite.py list --today        # ä»Šæ—¥ã®äºˆå®š

  python threads_sqlite.py add                 # æŠ•ç¨¿è¿½åŠ ï¼ˆå¯¾è©±å¼ï¼‰
  python threads_sqlite.py import --csv new.csv
  python threads_sqlite.py export --output backup.csv

  # PDCAåˆ†æ
  python threads_sqlite.py pdca                # éå»3æ—¥é–“
  python threads_sqlite.py pdca --days 7       # éå»7æ—¥é–“
        """
    )

    parser.add_argument(
        'command',
        nargs='?',
        default='post',
        choices=['post', 'list', 'add', 'import', 'export', 'pdca'],
        help='å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰'
    )

    parser.add_argument('--dry-run', action='store_true', help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--status', type=str, help='æŠ•ç¨¿ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ (pending/posted/failed)')
    parser.add_argument('--limit', type=int, default=20, help='è¡¨ç¤ºä»¶æ•°')
    parser.add_argument('--today', action='store_true', help='ä»Šæ—¥ã®æŠ•ç¨¿ã®ã¿')
    parser.add_argument('--tomorrow', action='store_true', help='æ˜æ—¥ã®æŠ•ç¨¿ã®ã¿')
    parser.add_argument('--csv', type=str, help='CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--output', type=str, help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--days', type=int, default=3, help='PDCAåˆ†æã®æ—¥æ•°')
    parser.add_argument('--datetime', type=str, help='æŠ•ç¨¿æ—¥æ™‚ (YYYY-MM-DD HH:MM)')
    parser.add_argument('--text', type=str, help='æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆ')
    parser.add_argument('--category', type=str, help='ã‚«ãƒ†ã‚´ãƒª')

    args = parser.parse_args()

    if args.dry_run:
        DRY_RUN = True

    # ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
    if args.command == 'post':
        # æŠ•ç¨¿ãƒ¢ãƒ¼ãƒ‰
        print("=" * 70)
        if DRY_RUN:
            print("Threads äºˆç´„æŠ•ç¨¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰)")
        else:
            print("Threads äºˆç´„æŠ•ç¨¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
        print("=" * 70)

        if not validate_config():
            print("\nâš ï¸  è¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)

        check_and_post()

        if DRY_RUN:
            print("\n" + "=" * 70)
            print("ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†")
            print("å®Ÿéš›ã«æŠ•ç¨¿ã™ã‚‹å ´åˆã¯ --dry-run ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å¤–ã—ã¦ãã ã•ã„")
            print("=" * 70)

    elif args.command == 'list':
        list_posts(args.status, args.limit, args.today, args.tomorrow)

    elif args.command == 'add':
        if args.datetime and args.text:
            add_post(args.datetime, args.text, args.category)
        else:
            print("ä½¿ç”¨æ–¹æ³•: python threads_sqlite.py add --datetime \"2025-11-02 08:00\" --text \"æŠ•ç¨¿å†…å®¹\" --category \"æ‹æ„›\"")

    elif args.command == 'import':
        if args.csv:
            import_from_csv(args.csv)
        else:
            print("ä½¿ç”¨æ–¹æ³•: python threads_sqlite.py import --csv posts.csv")

    elif args.command == 'export':
        output = args.output or 'posts_export.csv'
        export_to_csv(output, args.status)

    elif args.command == 'pdca':
        generate_pdca_report(args.days)


if __name__ == '__main__':
    if '-h' in sys.argv or '--help' in sys.argv:
        main()
        sys.exit(0)

    is_dry_run = '--dry-run' in sys.argv
    is_read_only = any(cmd in sys.argv for cmd in ['list', 'export', 'pdca', 'add', 'import'])

    if not is_dry_run and not is_read_only:
        if ACCESS_TOKEN == 'YOUR_ACCESS_TOKEN_HERE' or USER_ID == 'YOUR_USER_ID_HERE':
            print("\nâš ï¸  èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼")
            print("ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š")
            print("  export THREADS_ACCESS_TOKEN='your_token'")
            print("  export THREADS_USER_ID='your_user_id'")
            print("\nã¾ãŸã¯ã€å‹•ä½œç¢ºèªã ã‘ãªã‚‰ --dry-run ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š")
            print("  python threads_sqlite.py post --dry-run")
            sys.exit(1)

    main()
