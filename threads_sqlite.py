#!/usr/bin/env python3
"""
Threads API äºˆç´„æŠ•ç¨¿ + PDCAåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ (SQLiteç‰ˆ)
3æ—¥ã‚µã‚¤ã‚¯ãƒ«ã§PDCAã‚’å›ã™ã“ã¨ã«ç‰¹åŒ–
"""

import sqlite3
import csv
import time
import requests
import json
import argparse
import os
import sys
import re
from datetime import datetime, timedelta, timezone
from collections import defaultdict
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ï¼ˆæ—¢å­˜ã®ç’°å¢ƒå¤‰æ•°ã‚’ä¸Šæ›¸ãï¼‰
load_dotenv(override=True)


# ============================================
# è¨­å®šé …ç›®
# ============================================
ACCESS_TOKEN = os.getenv('THREADS_ACCESS_TOKEN', 'YOUR_ACCESS_TOKEN_HERE')
USER_ID = os.getenv('THREADS_USER_ID', 'YOUR_USER_ID_HERE')

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
DB_FILE = os.getenv('DB_FILE', 'threads.db')
PDCA_REPORT_FILE = 'pdca_report.md'
COMPETITOR_REPORT_FILE = 'competitor_report.md'

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆæŠ•ç¨¿é–“éš”ï¼‰
# scheduled_atã§æ™‚åˆ»æŒ‡å®šã™ã‚‹ãŸã‚ã€ã“ã®å€¤ã¯ä½¿ç”¨ã•ã‚Œãªããªã‚Šã¾ã—ãŸ
MIN_INTERVAL_SECONDS = 10  # 10ç§’ï¼ˆäºˆå‚™çš„ãªå¾…æ©Ÿæ™‚é–“ï¼‰

# ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰
DRY_RUN = False

# æŠ•ç¨¿å¾Œã«ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: True ã§å‰Šé™¤ï¼‰
DELETE_AFTER_POST = os.getenv('DELETE_AFTER_POST', 'true').lower() == 'true'

# é‹ç”¨é–‹å§‹æ—¥ï¼ˆã“ã®æ—¥ã‚’1æ—¥ç›®ã¨ã—ã¦æ—¥æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼‰
OPERATION_START_DATE = '2025-10-29'

# ============================================
# Threads API
# ============================================
API_BASE_URL = 'https://graph.threads.net/v1.0'


def get_db_connection():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å–å¾—"""
    conn = sqlite3.connect(DB_FILE)
    conn.row_factory = sqlite3.Row  # è¾æ›¸å½¢å¼ã§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«
    return conn


def migrate_db():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†"""
    conn = get_db_connection()
    cursor = conn.cursor()

    # æ—¢å­˜ã®ã‚«ãƒ©ãƒ ã‚’å–å¾—
    cursor.execute("PRAGMA table_info(posts)")
    columns = [column[1] for column in cursor.fetchall()]

    # thread_text ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ 
    if 'thread_text' not in columns:
        print("  â†’ thread_text ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ä¸­...")
        cursor.execute("ALTER TABLE posts ADD COLUMN thread_text TEXT")
        conn.commit()
        print("  âœ“ thread_text ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ã—ã¾ã—ãŸ")

    # topic ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ 
    if 'topic' not in columns:
        print("  â†’ topic ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ä¸­...")
        cursor.execute("ALTER TABLE posts ADD COLUMN topic TEXT")
        conn.commit()
        print("  âœ“ topic ã‚«ãƒ©ãƒ ã‚’è¿½åŠ ã—ã¾ã—ãŸ")

    conn.close()


def create_threads_post(text, reply_to_id=None):
    """Threads APIã§æŠ•ç¨¿ã‚’ä½œæˆï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å¯¾å¿œï¼‰"""
    global DRY_RUN

    if DRY_RUN:
        if reply_to_id:
            print(f"  â†’ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] ã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆä¸­... (è¿”ä¿¡å…ˆ: {reply_to_id})")
        else:
            print(f"  â†’ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] æŠ•ç¨¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆä¸­...")
        print(f"  â†’ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] å®Ÿéš›ã«ã¯æŠ•ç¨¿ã•ã‚Œã¾ã›ã‚“")
        time.sleep(0.5)
        fake_post_id = f"dry_run_{int(time.time())}"
        print(f"  âœ“ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] æŠ•ç¨¿æˆåŠŸï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰ï¼ (ID: {fake_post_id})")
        return fake_post_id

    try:
        create_url = f'{API_BASE_URL}/{USER_ID}/threads'
        # access_tokenã¯URLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãã®ä»–ã¯ãƒ•ã‚©ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦é€ä¿¡
        create_params = {
            'access_token': ACCESS_TOKEN
        }
        create_data = {
            'media_type': 'TEXT',
            'text': text
        }

        # ã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿ã®å ´åˆã¯reply_to_idã‚’è¿½åŠ 
        if reply_to_id:
            create_data['reply_to_id'] = reply_to_id
            print(f"  â†’ ã‚¹ãƒ¬ãƒƒãƒ‰ã‚³ãƒ³ãƒ†ãƒŠä½œæˆä¸­... (è¿”ä¿¡å…ˆ: {reply_to_id})")
        else:
            print(f"  â†’ ã‚³ãƒ³ãƒ†ãƒŠä½œæˆä¸­...")

        create_response = requests.post(create_url, params=create_params, data=create_data)
        create_response.raise_for_status()
        container_id = create_response.json().get('id')

        if not container_id:
            print(f"  âœ— ã‚³ãƒ³ãƒ†ãƒŠIDã®å–å¾—ã«å¤±æ•—")
            return None

        publish_url = f'{API_BASE_URL}/{USER_ID}/threads_publish'
        publish_params = {
            'access_token': ACCESS_TOKEN
        }
        publish_data = {
            'creation_id': container_id
        }

        print(f"  â†’ æŠ•ç¨¿å…¬é–‹ä¸­...")
        publish_response = requests.post(publish_url, params=publish_params, data=publish_data)
        publish_response.raise_for_status()

        post_id = publish_response.json().get('id')
        if post_id:
            if reply_to_id:
                print(f"  âœ“ ã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿æˆåŠŸï¼ (ID: {post_id})")
            else:
                print(f"  âœ“ æŠ•ç¨¿æˆåŠŸï¼ (ID: {post_id})")
            return post_id
        else:
            print(f"  âœ— æŠ•ç¨¿IDã®å–å¾—ã«å¤±æ•—")
            return None

    except requests.exceptions.RequestException as e:
        print(f"  âœ— API ã‚¨ãƒ©ãƒ¼: {e}")
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_detail = e.response.json()
                print(f"  âœ— ã‚¨ãƒ©ãƒ¼è©³ç´°: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
            except:
                print(f"  âœ— ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text[:200]}")
        return None


def get_user_profile():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ãªã©ï¼‰"""
    try:
        url = f'{API_BASE_URL}/{USER_ID}'
        params = {
            'fields': 'id,username,threads_profile_picture_url,threads_biography',
            'access_token': ACCESS_TOKEN
        }

        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚’å–å¾—
        threads_url = f'{API_BASE_URL}/{USER_ID}/threads'
        threads_params = {
            'fields': 'id',
            'limit': 1,
            'access_token': ACCESS_TOKEN
        }
        threads_response = requests.get(threads_url, params=threads_params)

        # Note: Threads APIã§ã¯ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã®ç›´æ¥å–å¾—ãŒã§ããªã„ãŸã‚ã€
        # ä»£ã‚ã‚Šã«æœ€è¿‘ã®æŠ•ç¨¿ã®ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‹ã‚‰æ¨å®šã—ã¾ã™

        return {
            'username': data.get('username', ''),
            'user_id': data.get('id', ''),
        }

    except requests.exceptions.RequestException as e:
        print(f"  âœ— ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def get_followers_count():
    """Threads Insights APIã‹ã‚‰ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚’å–å¾—"""
    try:
        url = f'{API_BASE_URL}/{USER_ID}/threads_insights'
        params = {
            'metric': 'followers_count',
            'access_token': ACCESS_TOKEN
        }

        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        if 'data' in data and len(data['data']) > 0:
            followers = data['data'][0]['total_value']['value']
            return followers
        return None

    except requests.exceptions.RequestException as e:
        print(f"  âœ— ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def save_daily_stats(date_str, followers_count, posts_count, total_likes, total_impressions):
    """æ—¥æ¬¡çµ±è¨ˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            INSERT OR REPLACE INTO daily_stats
            (date, followers_count, posts_count, total_likes, total_impressions)
            VALUES (?, ?, ?, ?, ?)
        """, (date_str, followers_count, posts_count, total_likes, total_impressions))
        conn.commit()
        conn.close()
        return True
    except Exception as e:
        print(f"  âœ— æ—¥æ¬¡çµ±è¨ˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def get_previous_day_stats(date_str):
    """æŒ‡å®šæ—¥ã®çµ±è¨ˆã‚’å–å¾—"""
    try:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT followers_count, posts_count, total_likes, total_impressions
            FROM daily_stats
            WHERE date = ?
        """, (date_str,))
        result = cursor.fetchone()
        conn.close()

        if result:
            return {
                'followers_count': result[0],
                'posts_count': result[1],
                'total_likes': result[2],
                'total_impressions': result[3]
            }
        return None
    except Exception as e:
        print(f"  âœ— å‰æ—¥çµ±è¨ˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def get_yesterday_posts_summary():
    """å‰æ—¥ã®æŠ•ç¨¿ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
    # æ—¥æœ¬æ™‚é–“ï¼ˆJST = UTC+9ï¼‰ã§å‰æ—¥ã®ç¯„å›²ã‚’å–å¾—
    from datetime import timezone, timedelta
    jst = timezone(timedelta(hours=9))
    today = datetime.now(jst).replace(hour=0, minute=0, second=0, microsecond=0)
    yesterday_start = (today - timedelta(days=1)).strftime('%Y-%m-%d %H:%M:%S')
    yesterday_end = today.strftime('%Y-%m-%d %H:%M:%S')

    # Threads APIã‹ã‚‰æœ€è¿‘ã®æŠ•ç¨¿ã‚’å–å¾—ï¼ˆå‰æ—¥åˆ†ã‚’ç¢ºå®Ÿã«å–å¾—ã™ã‚‹ãŸã‚å¤šã‚ã«å–å¾—ï¼‰
    try:
        url = f'{API_BASE_URL}/{USER_ID}/threads'
        params = {
            'fields': 'id,text,timestamp',
            'limit': 100,  # å‰æ—¥ã®æŠ•ç¨¿ã‚’ç¢ºå®Ÿã«å–å¾—ã™ã‚‹ãŸã‚100ä»¶ã«è¨­å®š
            'access_token': ACCESS_TOKEN
        }

        response = requests.get(url, params=params)
        response.raise_for_status()
        threads_data = response.json().get('data', [])

        print(f"  â†’ APIã‹ã‚‰{len(threads_data)}ä»¶ã®æŠ•ç¨¿ã‚’å–å¾—")

        # å‰æ—¥ã®æŠ•ç¨¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        yesterday_posts = []
        for post in threads_data:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆISO 8601å½¢å¼ï¼‰
            timestamp_str = post.get('timestamp', '')
            if timestamp_str:
                # 'Z'ã‚’'+00:00'ã«ç½®æ›ã—ã¦ãƒ‘ãƒ¼ã‚¹
                post_time = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                post_time_jst = post_time.astimezone(jst)

                post_time_str = post_time_jst.strftime('%Y-%m-%d %H:%M:%S')

                if yesterday_start <= post_time_str < yesterday_end:
                    yesterday_posts.append(post)
                    print(f"  â†’ å‰æ—¥ã®æŠ•ç¨¿ã‚’ç™ºè¦‹: {post['id']} ({post_time_str})")

        print(f"  â†’ å‰æ—¥ã®æŠ•ç¨¿: {len(yesterday_posts)}ä»¶")

        # å„æŠ•ç¨¿ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        total_likes = 0
        total_views = 0

        for i, post in enumerate(yesterday_posts, 1):
            print(f"  â†’ [{i}/{len(yesterday_posts)}] æŠ•ç¨¿ {post['id']} ã®åˆ†æãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
            insights = get_post_insights(post['id'])
            if insights:
                total_likes += insights['likes']
                total_views += insights['views']
                print(f"     ã„ã„ã­: {insights['likes']}, è¡¨ç¤º: {insights['views']}")
            time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

        return {
            'post_count': len(yesterday_posts),
            'total_likes': total_likes,
            'total_views': total_views,
        }

    except Exception as e:
        print(f"  âœ— å‰æ—¥æŠ•ç¨¿ã‚µãƒãƒªãƒ¼å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_detail = e.response.json()
                print(f"  âœ— ã‚¨ãƒ©ãƒ¼è©³ç´°: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
            except:
                print(f"  âœ— ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {e.response.text[:500]}")
        return None


def get_post_insights(threads_post_id):
    """æŠ•ç¨¿ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    try:
        url = f'{API_BASE_URL}/{threads_post_id}'
        params = {
            'fields': 'id,text,timestamp,permalink',
            'access_token': ACCESS_TOKEN
        }

        response = requests.get(url, params=params)
        response.raise_for_status()
        data = response.json()

        insights_url = f'{API_BASE_URL}/{threads_post_id}/insights'
        insights_params = {
            'metric': 'views,likes,replies,reposts,quotes',
            'access_token': ACCESS_TOKEN
        }

        insights_response = requests.get(insights_url, params=insights_params)

        insights_data = {}
        if insights_response.status_code == 200:
            insights = insights_response.json().get('data', [])
            for metric in insights:
                insights_data[metric['name']] = metric.get('values', [{}])[0].get('value', 0)

        result = {
            'threads_post_id': threads_post_id,
            'text': data.get('text', ''),
            'timestamp': data.get('timestamp', ''),
            'permalink': data.get('permalink', ''),
            'views': insights_data.get('views', 0),
            'likes': insights_data.get('likes', 0),
            'replies': insights_data.get('replies', 0),
            'reposts': insights_data.get('reposts', 0),
            'quotes': insights_data.get('quotes', 0),
        }

        total_engagement = (
            result['likes'] +
            result['replies'] +
            result['reposts'] +
            result['quotes']
        )
        result['engagement'] = total_engagement
        result['engagement_rate'] = (
            (total_engagement / result['views'] * 100)
            if result['views'] > 0 else 0
        )

        return result

    except requests.exceptions.RequestException as e:
        print(f"  âœ— åˆ†æãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None


# ============================================
# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ
# ============================================

def get_pending_posts():
    """æœªæŠ•ç¨¿ã§æŠ•ç¨¿å¯èƒ½ãªã‚‚ã®ã‚’å–å¾—ï¼ˆ1ä»¶ã®ã¿ã€1æ—¥25ä»¶åˆ¶é™ï¼‰"""
    conn = get_db_connection()
    cursor = conn.cursor()

    # æ—¥æœ¬æ™‚é–“ï¼ˆJST = UTC+9ï¼‰ã§ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
    from datetime import timezone, timedelta
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst)
    current_time = now.strftime('%Y-%m-%d %H:%M:%S')
    today_start = now.replace(hour=0, minute=0, second=0, microsecond=0).strftime('%Y-%m-%d %H:%M:%S')

    # ä»Šæ—¥ã®æŠ•ç¨¿æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆThreads APIåˆ¶é™ï¼š1æ—¥25ä»¶ï¼‰
    cursor.execute("""
        SELECT COUNT(*) as count
        FROM posts
        WHERE status = 'posted'
          AND posted_at >= ?
    """, (today_start,))

    today_count = cursor.fetchone()['count']

    # 25ä»¶ã«é”ã—ã¦ã„ã‚‹å ´åˆã¯æŠ•ç¨¿ã—ãªã„
    if today_count >= 25:
        conn.close()
        print(f"âš ï¸  æœ¬æ—¥ã®æŠ•ç¨¿æ•°ãŒä¸Šé™ï¼ˆ25ä»¶ï¼‰ã«é”ã—ã¦ã„ã¾ã™ã€‚æ˜æ—¥ã¾ã§å¾…æ©Ÿã—ã¾ã™ã€‚")
        return []

    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼š1å›ã®å®Ÿè¡Œã§1ä»¶ã®ã¿å–å¾—
    # scheduled_at ãŒç¾åœ¨æ™‚åˆ»ä»¥å‰ã®æœªæŠ•ç¨¿åˆ†ã‚’å¤ã„é †ã«å–å¾—
    cursor.execute("""
        SELECT id, csv_id, scheduled_at, text, category, topic, thread_text
        FROM posts
        WHERE status = 'pending'
          AND scheduled_at <= ?
        ORDER BY scheduled_at
        LIMIT 1
    """, (current_time,))

    posts = cursor.fetchall()
    conn.close()

    return [dict(row) for row in posts]


def save_to_posted_history(csv_id, posted_at):
    """æŠ•ç¨¿å±¥æ­´ã‚’ posted_history.csv ã«è¿½è¨˜"""
    history_file = 'posted_history.csv'

    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆ
    file_exists = os.path.exists(history_file)

    try:
        with open(history_file, 'a', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            if not file_exists:
                writer.writerow(['csv_id', 'posted_at'])
            writer.writerow([csv_id, posted_at])
        print(f"  âœ“ posted_history.csv ã«è¨˜éŒ²ã—ã¾ã—ãŸ (csv_id: {csv_id})")
    except Exception as e:
        print(f"  âš ï¸  posted_history.csv ã¸ã®æ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


def mark_as_posted(post_id, threads_post_id, csv_id=None):
    """æŠ•ç¨¿ã‚’æŠ•ç¨¿æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
    conn = get_db_connection()
    cursor = conn.cursor()

    # JST (UTC+9) ã§ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
    jst = timezone(timedelta(hours=9))
    posted_at = datetime.now(jst).replace(tzinfo=None)  # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’å‰Šé™¤ã—ã¦JSTæ™‚åˆ»ã¨ã—ã¦ä¿å­˜

    cursor.execute("""
        UPDATE posts
        SET status = 'posted',
            threads_post_id = ?,
            posted_at = ?,
            updated_at = CURRENT_TIMESTAMP
        WHERE id = ?
    """, (threads_post_id, posted_at, post_id))

    conn.commit()
    conn.close()

    # posted_history.csv ã«è¿½è¨˜
    if csv_id:
        save_to_posted_history(csv_id, posted_at)


def mark_as_failed(post_id, error_message):
    """æŠ•ç¨¿ã‚’å¤±æ•—ã¨ã—ã¦ãƒãƒ¼ã‚¯"""
    conn = get_db_connection()
    cursor = conn.cursor()

    cursor.execute("""
        UPDATE posts
        SET status = 'failed',
            error_message = ?,
            updated_at = CURRENT_TIMESTAMP
        WHERE id = ?
    """, (error_message, post_id))

    conn.commit()
    conn.close()


def save_analytics(post_id, analytics_data):
    """åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    conn = get_db_connection()
    cursor = conn.cursor()

    # JST (UTC+9) ã§ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
    jst = timezone(timedelta(hours=9))
    fetched_at = datetime.now(jst).replace(tzinfo=None)

    cursor.execute("""
        INSERT INTO analytics (
            post_id, threads_post_id,
            views, likes, replies, reposts, quotes,
            engagement, engagement_rate,
            permalink, fetched_at
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        post_id,
        analytics_data['threads_post_id'],
        analytics_data['views'],
        analytics_data['likes'],
        analytics_data['replies'],
        analytics_data['reposts'],
        analytics_data['quotes'],
        analytics_data['engagement'],
        analytics_data['engagement_rate'],
        analytics_data['permalink'],
        fetched_at
    ))

    conn.commit()
    conn.close()


def get_recent_posts(days=3):
    """éå»Næ—¥é–“ã®æŠ•ç¨¿æ¸ˆã¿æŠ•ç¨¿ã‚’å–å¾—"""
    conn = get_db_connection()
    cursor = conn.cursor()

    # JST (UTC+9) ã§ç¾åœ¨æ™‚åˆ»ã‚’å–å¾—
    jst = timezone(timedelta(hours=9))
    now_jst = datetime.now(jst).replace(tzinfo=None)
    cutoff_date = (now_jst - timedelta(days=days)).strftime('%Y-%m-%d %H:%M:%S')

    cursor.execute("""
        SELECT id, csv_id, text, posted_at, threads_post_id, category, scheduled_at
        FROM posts
        WHERE status = 'posted'
          AND posted_at >= ?
        ORDER BY posted_at DESC
    """, (cutoff_date,))

    posts = cursor.fetchall()
    conn.close()

    return [dict(row) for row in posts]


# ============================================
# æŠ•ç¨¿ç®¡ç†ã‚³ãƒãƒ³ãƒ‰
# ============================================

def add_post(scheduled_at, text, category=None):
    """æ–°ã—ã„æŠ•ç¨¿ã‚’è¿½åŠ """
    conn = get_db_connection()
    cursor = conn.cursor()

    # æœ€æ–°ã®csv_idã‚’å–å¾—
    cursor.execute("SELECT MAX(CAST(csv_id AS INTEGER)) FROM posts WHERE csv_id GLOB '[0-9]*'")
    result = cursor.fetchone()
    max_id = result[0] if result[0] else 0
    new_csv_id = str(max_id + 1)

    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ
    char_count = len(text)
    has_emoji = any(ord(c) > 127 for c in text)

    # ã‚«ãƒ†ã‚´ãƒªè‡ªå‹•æ¤œå‡º
    if not category:
        category = detect_category(text)

    cursor.execute("""
        INSERT INTO posts (
            csv_id, scheduled_at, text, status,
            category, char_count, has_emoji
        ) VALUES (?, ?, ?, 'pending', ?, ?, ?)
    """, (new_csv_id, scheduled_at, text, category, char_count, has_emoji))

    conn.commit()
    post_id = cursor.lastrowid
    conn.close()

    print(f"âœ… æŠ•ç¨¿ã‚’è¿½åŠ ã—ã¾ã—ãŸ (ID: {new_csv_id})")
    print(f"   æ—¥æ™‚: {scheduled_at}")
    print(f"   ã‚«ãƒ†ã‚´ãƒª: {category}")
    print(f"   æ–‡å­—æ•°: {char_count}æ–‡å­—")

    return post_id


def detect_category(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æ¨å®š"""
    keywords = {
        'æ‹æ„›': ['å½¼æ°', 'å½¼å¥³', 'æ‹æ„›', 'ãƒãƒƒãƒãƒ³ã‚°', 'å‡ºä¼šã„', 'å¥½ã', 'ãƒ‡ãƒ¼ãƒˆ', 'çµå©š'],
        'ä»•äº‹': ['è»¢è·', 'ä»•äº‹', 'è·å ´', 'ä¼šç¤¾', 'ä¸Šå¸', 'çµ¦æ–™', 'æ®‹æ¥­', 'ã‚­ãƒ£ãƒªã‚¢'],
        'ãŠé‡‘': ['è²¯é‡‘', 'ãŠé‡‘', 'ç¯€ç´„', 'NISA', 'æŠ•è³‡', 'ã‚µãƒ–ã‚¹ã‚¯', 'ã‚¯ãƒ¬ã‚«', 'æµªè²»'],
        'ãƒ¡ãƒ³ã‚¿ãƒ«': ['HSP', 'ç¹Šç´°', 'è‡ªå·±è‚¯å®šæ„Ÿ', 'ã‚¹ãƒˆãƒ¬ã‚¹', 'ç–²ã‚Œ', 'ä¸å®‰', 'ãƒ¡ãƒ³ã‚¿ãƒ«'],
        'å ã„': ['å ã„', 'æœˆæ˜Ÿåº§', 'æ•°ç§˜', 'ã‚¿ãƒ­ãƒƒãƒˆ', 'ãƒ›ãƒ­ã‚¹ã‚³ãƒ¼ãƒ—', 'ã‚¹ãƒ”ãƒªãƒãƒ¥ã‚¢ãƒ«']
    }

    for category, words in keywords.items():
        if any(word in text for word in words):
            return category

    return 'ãã®ä»–'


def list_posts(status=None, limit=20, today=False, tomorrow=False):
    """æŠ•ç¨¿ä¸€è¦§ã‚’è¡¨ç¤º"""
    conn = get_db_connection()
    cursor = conn.cursor()

    query = "SELECT id, csv_id, scheduled_at, substr(text, 1, 50) as preview, status, category FROM posts"
    params = []

    conditions = []
    if status:
        conditions.append("status = ?")
        params.append(status)

    if today:
        conditions.append("DATE(scheduled_at) = DATE('now', 'localtime')")
    elif tomorrow:
        conditions.append("DATE(scheduled_at) = DATE('now', 'localtime', '+1 day')")

    if conditions:
        query += " WHERE " + " AND ".join(conditions)

    query += f" ORDER BY scheduled_at DESC LIMIT {limit}"

    cursor.execute(query, params)
    posts = cursor.fetchall()
    conn.close()

    if not posts:
        print("æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    print(f"\næŠ•ç¨¿ä¸€è¦§ ({len(posts)}ä»¶):")
    print("-" * 100)

    for post in posts:
        post_dict = dict(post)
        print(f"[{post_dict['csv_id']}] {post_dict['scheduled_at']} | {post_dict['status']:8s} | {post_dict['category']:8s}")
        print(f"      {post_dict['preview']}...")
        print()


def import_from_csv(csv_file):
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ•ç¨¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆposted_history.csv ã‚’è€ƒæ…®ï¼‰"""
    if not os.path.exists(csv_file):
        print(f"âœ— ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_file}")
        return

    # posted_history.csv ã‹ã‚‰æŠ•ç¨¿æ¸ˆã¿ csv_id ã‚’èª­ã¿è¾¼ã‚€
    posted_ids = {}  # {csv_id: posted_at}
    history_file = 'posted_history.csv'
    if os.path.exists(history_file):
        with open(history_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                csv_id = row.get('csv_id', '').strip()
                posted_at = row.get('posted_at', '').strip()
                if csv_id:
                    posted_ids[csv_id] = posted_at
        print(f"ğŸ“ posted_history.csv ã‹ã‚‰ {len(posted_ids)} ä»¶ã®æŠ•ç¨¿æ¸ˆã¿è¨˜éŒ²ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    conn = get_db_connection()
    cursor = conn.cursor()

    imported = 0
    skipped_posted = 0  # æŠ•ç¨¿æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
    skipped_error = 0   # ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—

    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)

        for row in reader:
            csv_id = row.get('id', '').strip()
            datetime_str = row.get('datetime', '').strip()
            text = row.get('text', '').strip()
            category = row.get('category', '').strip() or None
            topic = row.get('topic', '').strip() or None
            thread_text = row.get('thread_text', '').strip() or None

            if not csv_id or not datetime_str or not text:
                skipped_error += 1
                continue

            # posted_history.csv ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if csv_id in posted_ids:
                skipped_posted += 1
                continue

            try:
                scheduled_at = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M')
                char_count = len(text)
                has_emoji = any(ord(c) > 127 for c in text)

                if not category:
                    category = detect_category(text)

                cursor.execute("""
                    INSERT INTO posts (
                        csv_id, scheduled_at, text, thread_text, status,
                        category, topic, char_count, has_emoji
                    ) VALUES (?, ?, ?, ?, 'pending', ?, ?, ?, ?)
                """, (csv_id, scheduled_at, text, thread_text, category, topic, char_count, has_emoji))

                imported += 1

            except sqlite3.IntegrityError:
                skipped_error += 1
            except Exception as e:
                print(f"âœ— ã‚¨ãƒ©ãƒ¼ (ID: {csv_id}): {e}")
                skipped_error += 1

    conn.commit()
    conn.close()

    print(f"âœ… ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†:")
    print(f"   ã‚¤ãƒ³ãƒãƒ¼ãƒˆ: {imported}ä»¶")
    print(f"   æŠ•ç¨¿æ¸ˆã¿ã‚¹ã‚­ãƒƒãƒ—: {skipped_posted}ä»¶")
    if skipped_error > 0:
        print(f"   ã‚¨ãƒ©ãƒ¼ã‚¹ã‚­ãƒƒãƒ—: {skipped_error}ä»¶")


def export_to_csv(output_file, status=None):
    """æŠ•ç¨¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    conn = get_db_connection()
    cursor = conn.cursor()

    query = "SELECT csv_id, scheduled_at, text, status, category FROM posts"
    params = []

    if status:
        query += " WHERE status = ?"
        params.append(status)

    query += " ORDER BY scheduled_at"

    cursor.execute(query, params)
    posts = cursor.fetchall()
    conn.close()

    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['id', 'datetime', 'text', 'status', 'category'])

        for post in posts:
            post_dict = dict(post)
            dt = datetime.fromisoformat(post_dict['scheduled_at']).strftime('%Y-%m-%d %H:%M')
            writer.writerow([
                post_dict['csv_id'],
                dt,
                post_dict['text'],
                post_dict['status'],
                post_dict['category']
            ])

    print(f"âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {len(posts)}ä»¶ â†’ {output_file}")


# ============================================
# æŠ•ç¨¿å‡¦ç†
# ============================================

def load_posted_history():
    """posted_history.csv ã‹ã‚‰æŠ•ç¨¿æ¸ˆã¿ã® csv_id ã‚’å–å¾—"""
    posted_ids = set()
    history_file = 'posted_history.csv'

    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    posted_ids.add(str(row['csv_id']))
            print(f"ğŸ“‹ æŠ•ç¨¿å±¥æ­´: {len(posted_ids)}ä»¶ã®æŠ•ç¨¿ã‚’ç¢ºèª")
        except Exception as e:
            print(f"âš ï¸  æŠ•ç¨¿å±¥æ­´ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    return posted_ids


def check_text_duplicate(text):
    """åŒã˜ãƒ†ã‚­ã‚¹ãƒˆãŒæ—¢ã«æŠ•ç¨¿ã•ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯"""
    conn = get_db_connection()
    cursor = conn.cursor()

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã§åŒã˜ãƒ†ã‚­ã‚¹ãƒˆã®æŠ•ç¨¿æ¸ˆã¿ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢
    cursor.execute("""
        SELECT csv_id, posted_at
        FROM posts
        WHERE status = 'posted'
          AND text = ?
        LIMIT 1
    """, (text,))

    result = cursor.fetchone()
    conn.close()

    if result:
        return {
            'is_duplicate': True,
            'csv_id': result['csv_id'],
            'posted_at': result['posted_at']
        }

    return {'is_duplicate': False}


def delete_posted_record(post_id):
    """æŠ•ç¨¿æ¸ˆã¿ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‰Šé™¤"""
    conn = get_db_connection()
    cursor = conn.cursor()

    cursor.execute("DELETE FROM posts WHERE id = ?", (post_id,))

    conn.commit()
    conn.close()
    print(f"  âœ“ ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‰Šé™¤ã—ã¾ã—ãŸ (ID: {post_id})")


def generate_daily_report_text(summary):
    """å‰æ—¥ã®ãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    import random
    from datetime import timezone, timedelta

    post_count = summary['post_count']
    total_likes = summary['total_likes']
    total_views = summary['total_views']
    followers_count = summary.get('followers_count')
    followers_diff = summary.get('followers_diff')

    # é‹ç”¨æ—¥æ•°ã‚’è¨ˆç®—
    jst = timezone(timedelta(hours=9))
    today = datetime.now(jst)
    start_date = datetime.strptime(OPERATION_START_DATE, '%Y-%m-%d').replace(tzinfo=jst)
    days_since_start = (today.date() - start_date.date()).days + 1  # é–‹å§‹æ—¥ã‚’1æ—¥ç›®ã¨ã™ã‚‹

    # æˆæœã«å¿œã˜ã¦å‰å‘ããªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¤‰ãˆã‚‹
    if total_likes >= 100:
        motivation = random.choice([
            "ã„ã„æ„Ÿã˜ï¼ã“ã®èª¿å­ã§é ‘å¼µã‚‹ãï¼",
            "é †èª¿ï¼ä»Šæ—¥ã‚‚æ¥½ã—ãç™ºä¿¡ã—ã¦ã„ã“ã†âœ¨",
            "å¬‰ã—ã„ï¼ã‚‚ã£ã¨ä¼¸ã°ã—ã¦ã„ããï¼",
            "æœ€é«˜ï¼ä»Šæ—¥ã‚‚é ‘å¼µã‚ã†ğŸ’ª"
        ])
    elif total_likes >= 50:
        motivation = random.choice([
            "å°‘ã—ãšã¤ä¼¸ã³ã¦ã‚‹ï¼ä»Šæ—¥ã‚‚é ‘å¼µã‚‹ãï¼",
            "ã„ã„æ„Ÿã˜ï¼ç¶™ç¶šãŒåŠ›ã«ãªã£ã¦ããŸâœ¨",
            "ç€å®Ÿã«æˆé•·ä¸­ï¼ä»Šæ—¥ã‚‚æ¥½ã—ãç™ºä¿¡ã—ã‚ˆã†ï¼",
            "æ‰‹å¿œãˆã‚ã‚Šï¼ã‚‚ã£ã¨é ‘å¼µã‚ã†ğŸ’ª"
        ])
    elif total_likes >= 20:
        motivation = random.choice([
            "ã¾ã ã¾ã ã“ã‚Œã‹ã‚‰ï¼ã‚‚ã£ã¨é ‘å¼µã‚‹ãï¼",
            "å°‘ã—ãšã¤å‰é€²ï¼ä»Šæ—¥ã‚‚ç¶šã‘ã‚ˆã†âœ¨",
            "æˆé•·ä¸­ï¼ç¶™ç¶šã‚ã‚‹ã®ã¿ğŸ’ª",
            "è«¦ã‚ãªã„ï¼ä»Šæ—¥ã‚‚é ‘å¼µã‚‹ãï¼"
        ])
    else:
        motivation = random.choice([
            "ã“ã‚Œã‹ã‚‰ï¼ã‚‚ã£ã¨ã‚‚ã£ã¨é ‘å¼µã‚‹ãï¼",
            "ã¾ã ã¾ã ï¼ä»Šæ—¥ã‚‚å…¨åŠ›ã§ç™ºä¿¡ã—ã‚ˆã†ğŸ’ª",
            "è² ã‘ãªã„ï¼ç¶™ç¶šã—ã¦ä¼¸ã°ã—ã¦ã„ããâœ¨",
            "ã¾ã å§‹ã¾ã£ãŸã°ã‹ã‚Šï¼ä»Šæ—¥ã‚‚é ‘å¼µã‚‹ï¼"
        ])

    # å¹³å‡ã„ã„ã­æ•°ã‚’è¨ˆç®—
    avg_likes = total_likes / post_count if post_count > 0 else 0

    # ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã®è¡Œã‚’ä½œæˆ
    followers_line = ""
    if followers_count is not None:
        if followers_diff is not None and followers_diff != 0:
            if followers_diff > 0:
                followers_line = f"ã€ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã€‘{followers_count}äººï¼ˆå‰æ—¥æ¯”+{followers_diff}äººğŸ‘†ï¼‰\n"
            else:
                followers_line = f"ã€ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã€‘{followers_count}äººï¼ˆå‰æ—¥æ¯”{followers_diff}äººï¼‰\n"
        else:
            followers_line = f"ã€ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ã€‘{followers_count}äºº\n"

    report_text = f"""ãŠã¯ã‚ˆã†â˜€ï¸
é‹ç”¨é–‹å§‹ã—ã¦{days_since_start}æ—¥ç›®ã®æˆæœå ±å‘Šï¼

ã€æŠ•ç¨¿æ•°ã€‘{post_count}æŠ•ç¨¿
ã€ã„ã„ã­ã€‘{total_likes}ã„ã„ã­ï¼ˆå¹³å‡{avg_likes:.1f}ï¼‰
ã€ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³ã€‘{total_views:,}å›
{followers_line}{motivation}"""

    return report_text


def create_daily_report():
    """æ¯æœã®ãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿ã‚’ä½œæˆ"""
    global DRY_RUN
    from datetime import timezone, timedelta

    print("\n" + "="*70)
    print("ğŸ“Š æ¯æœã®ãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿")
    print("="*70)

    # å‰æ—¥ã®æŠ•ç¨¿ã‚µãƒãƒªãƒ¼ã‚’å–å¾—
    print("\nå‰æ—¥ã®æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    summary = get_yesterday_posts_summary()

    if not summary:
        print("âš ï¸  å‰æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    if summary['post_count'] == 0:
        print("âš ï¸  å‰æ—¥ã®æŠ•ç¨¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # ç¾åœ¨ã®ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚’å–å¾—
    print("\nãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚’å–å¾—ä¸­...")
    current_followers = get_followers_count()

    if current_followers is not None:
        print(f"  â†’ ç¾åœ¨ã®ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°: {current_followers}äºº")
        summary['followers_count'] = current_followers

        # å‰æ—¥ï¼ˆ2æ—¥å‰ï¼‰ã®çµ±è¨ˆã‚’å–å¾—ã—ã¦æ¯”è¼ƒ
        jst = timezone(timedelta(hours=9))
        yesterday = datetime.now(jst) - timedelta(days=1)
        yesterday_str = yesterday.strftime('%Y-%m-%d')

        prev_stats = get_previous_day_stats(yesterday_str)

        if prev_stats and prev_stats['followers_count'] is not None:
            followers_diff = current_followers - prev_stats['followers_count']
            summary['followers_diff'] = followers_diff
            print(f"  â†’ å‰æ—¥æ¯”: {'+' if followers_diff >= 0 else ''}{followers_diff}äºº")
        else:
            summary['followers_diff'] = None
            print(f"  â†’ å‰æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆåˆå›è¨˜éŒ²ï¼‰")

        # æœ¬æ—¥ã®çµ±è¨ˆã‚’ä¿å­˜
        today = datetime.now(jst)
        today_str = today.strftime('%Y-%m-%d')
        save_daily_stats(
            today_str,
            current_followers,
            summary['post_count'],
            summary['total_likes'],
            summary['total_views']
        )
        print(f"  âœ“ æœ¬æ—¥ã®çµ±è¨ˆã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    else:
        print("  âš ï¸  ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        summary['followers_count'] = None
        summary['followers_diff'] = None

    # ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
    report_text = generate_daily_report_text(summary)

    print(f"\nç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ:\n{report_text}\n")

    if DRY_RUN:
        print("[ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] å®Ÿéš›ã«ã¯æŠ•ç¨¿ã•ã‚Œã¾ã›ã‚“")
        return

    # ãƒ¬ãƒãƒ¼ãƒˆã‚’æŠ•ç¨¿
    print("ãƒ¬ãƒãƒ¼ãƒˆã‚’æŠ•ç¨¿ä¸­...")
    threads_post_id = create_threads_post(report_text)

    if threads_post_id:
        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿å®Œäº†ï¼")
    else:
        print(f"âœ— ãƒ¬ãƒãƒ¼ãƒˆæŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸ")


def update_learnings(entry_text):
    """å­¦ç¿’ãƒ­ã‚°ã‚’æ›´æ–°"""
    from datetime import timezone, timedelta

    learnings_file = 'learnings.md'
    jst = timezone(timedelta(hours=9))
    today = datetime.now(jst).strftime('%Y-%m-%d')

    # æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªãƒ¼
    new_entry = f"""## {today}

### ğŸ“Š æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
[æ‰‹å‹•ã§è¿½è¨˜]

### ğŸ’¡ ä»®èª¬
{entry_text}

### âœ…/âŒ çµæœ
[çµæœã‚’è¨˜å…¥]

### ğŸ¯ æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
[æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨˜å…¥]

---

"""

    try:
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        if os.path.exists(learnings_file):
            with open(learnings_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # æœ€åˆã® "---" ã®å¾Œã«æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’æŒ¿å…¥
            if '---\n\n## ' in content:
                parts = content.split('---\n\n## ', 1)
                updated_content = parts[0] + '---\n\n' + new_entry + '## ' + parts[1]
            else:
                # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒé•ã†å ´åˆã¯æœ€å¾Œã«è¿½åŠ 
                updated_content = content + '\n' + new_entry
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯æ–°è¦ä½œæˆ
            updated_content = f"""# Threadsé‹ç”¨ å­¦ç¿’ãƒ­ã‚°

> ä»®èª¬æ¤œè¨¼ã¨æ”¹å–„ã®è¨˜éŒ²ã€‚æœ€æ–°30æ—¥åˆ†ã‚’ä¿æŒã€‚

---

{new_entry}"""

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        with open(learnings_file, 'w', encoding='utf-8') as f:
            f.write(updated_content)

        print(f"âœ… å­¦ç¿’ãƒ­ã‚°ã‚’æ›´æ–°ã—ã¾ã—ãŸ: {learnings_file}")
        print(f"   æ—¥ä»˜: {today}")
        return True

    except Exception as e:
        print(f"âœ— å­¦ç¿’ãƒ­ã‚°æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def check_and_post():
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æŠ•ç¨¿"""
    global DRY_RUN
    from datetime import timezone, timedelta

    # ç¾åœ¨æ™‚åˆ»ã‚’è¡¨ç¤º
    jst = timezone(timedelta(hours=9))
    current_time_jst = datetime.now(jst).strftime('%Y-%m-%d %H:%M:%S')
    print(f"\nğŸ“… ç¾åœ¨æ™‚åˆ»ï¼ˆJSTï¼‰: {current_time_jst}")

    # æŠ•ç¨¿æ¸ˆã¿å±¥æ­´ã‚’èª­ã¿è¾¼ã¿
    posted_ids = load_posted_history()

    posts = get_pending_posts()

    if not posts:
        print("æŠ•ç¨¿å¾…ã¡ã®é …ç›®ã¯ã‚ã‚Šã¾ã›ã‚“")
        return

    print(f"\nâœ… æŠ•ç¨¿å¯¾è±¡: {len(posts)}ä»¶")
    print(f"æ™‚åˆ»ç¯„å›²: {posts[0]['scheduled_at']} ã€œ {posts[-1]['scheduled_at']}")

    if DRY_RUN:
        print("\n[ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] ä»¥ä¸‹ã®æŠ•ç¨¿ãŒå®Ÿè¡Œã•ã‚Œã¾ã™:\n")

    for i, post in enumerate(posts):
        post_id = post['id']
        csv_id = str(post['csv_id'])
        text = post['text']
        scheduled_at = post['scheduled_at']
        category = post.get('category', 'æœªåˆ†é¡')
        thread_text = post.get('thread_text', '')

        # scheduled_atã¾ã§å¾…æ©Ÿï¼ˆæœªæ¥ã®å ´åˆã®ã¿ï¼‰
        jst = timezone(timedelta(hours=9))
        scheduled_dt = datetime.fromisoformat(scheduled_at.replace(' ', 'T')).replace(tzinfo=jst)
        current_time = datetime.now(jst)

        if scheduled_dt > current_time:
            wait_seconds = (scheduled_dt - current_time).total_seconds()

            # å¾…æ©Ÿæ™‚é–“ã®ä¸Šé™ãƒã‚§ãƒƒã‚¯ï¼ˆ4æ™‚é–“ï¼‰
            max_wait = 4 * 60 * 60  # 4æ™‚é–“

            if wait_seconds > max_wait:
                print(f"\n[{i+1}/{len(posts)}] æŠ•ç¨¿ID: {csv_id} | {scheduled_at} | [{category}]")
                print(f"  âš ï¸  scheduled_at ({scheduled_at}) ãŒé ã™ãã‚‹ãŸã‚æ¬¡å›å®Ÿè¡Œã«å»¶æœŸ")
                continue

            wait_minutes = int(wait_seconds / 60)
            wait_secs = int(wait_seconds % 60)
            print(f"\n[{i+1}/{len(posts)}] æŠ•ç¨¿ID: {csv_id} | {scheduled_at} | [{category}]")
            print(f"  â° scheduled_at ({scheduled_at}) ã¾ã§ {wait_minutes}åˆ†{wait_secs}ç§’å¾…æ©Ÿ...")
            time.sleep(wait_seconds)
        else:
            print(f"\n[{i+1}/{len(posts)}] æŠ•ç¨¿ID: {csv_id} | {scheduled_at} | [{category}]")

        print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text[:80]}{'...' if len(text) > 80 else ''}")
        if thread_text:
            print(f"  ã‚¹ãƒ¬ãƒƒãƒ‰: ã‚ã‚Šï¼ˆ{len(thread_text)}æ–‡å­—ï¼‰")

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯1: posted_history.csv
        if csv_id in posted_ids:
            print(f"  âš ï¸  ã™ã§ã«æŠ•ç¨¿æ¸ˆã¿ï¼ˆposted_history.csvã«è¨˜éŒ²ã‚ã‚Šï¼‰")
            print(f"  â†’ ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã¾ã™")
            delete_posted_record(post_id)
            continue

        # é‡è¤‡ãƒã‚§ãƒƒã‚¯2: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
        duplicate_check = check_text_duplicate(text)
        if duplicate_check['is_duplicate']:
            print(f"  âš ï¸  åŒã˜å†…å®¹ãŒæ—¢ã«æŠ•ç¨¿æ¸ˆã¿ã§ã™")
            print(f"     éå»ã®æŠ•ç¨¿: csv_id={duplicate_check['csv_id']}, æŠ•ç¨¿æ—¥æ™‚={duplicate_check['posted_at']}")
            print(f"  â†’ ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã¾ã™")
            delete_posted_record(post_id)
            continue

        if DRY_RUN:
            print(f"  â†’ [ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] å®Ÿéš›ã«ã¯æŠ•ç¨¿ã•ã‚Œã¾ã›ã‚“")
            continue

        # ãƒ¡ã‚¤ãƒ³æŠ•ç¨¿ã‚’ä½œæˆ
        threads_post_id = create_threads_post(text)

        if threads_post_id:
            # ã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿ãŒã‚ã‚‹å ´åˆã¯ç¶šã‘ã¦æŠ•ç¨¿
            if thread_text and thread_text.strip():
                print(f"  â†’ ã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿ã‚’ä½œæˆä¸­...")
                time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’é¿ã‘ã‚‹ãŸã‚å°‘ã—å¾…æ©Ÿ
                thread_post_id = create_threads_post(thread_text, reply_to_id=threads_post_id)
                if not thread_post_id:
                    print(f"  âš ï¸  ã‚¹ãƒ¬ãƒƒãƒ‰æŠ•ç¨¿ã«å¤±æ•—ã—ã¾ã—ãŸãŒã€ãƒ¡ã‚¤ãƒ³æŠ•ç¨¿ã¯æˆåŠŸ")

            # æŠ•ç¨¿æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯ï¼ˆposted_history.csv ã¸ã®è¨˜éŒ²ã‚’å«ã‚€ï¼‰
            mark_as_posted(post_id, threads_post_id, csv_id)

            # æŠ•ç¨¿å¾Œã«ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œï¼‰
            if DELETE_AFTER_POST:
                print(f"  â†’ æŠ•ç¨¿æ¸ˆã¿ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã¾ã™ï¼ˆå†æŠ•ç¨¿é˜²æ­¢ï¼‰")
                delete_posted_record(post_id)
        else:
            mark_as_failed(post_id, "APIæŠ•ç¨¿ã‚¨ãƒ©ãƒ¼")


# ============================================
# PDCAåˆ†æ
# ============================================

def generate_pdca_report(days=3):
    """PDCAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
    print("\n" + "="*70)
    print(f"ğŸ“Š éå»{days}æ—¥é–“ã®PDCAãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
    print("="*70)

    recent_posts = get_recent_posts(days)

    if not recent_posts:
        print(f"âš ï¸  éå»{days}æ—¥é–“ã®æŠ•ç¨¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    print(f"\néå»{days}æ—¥é–“ã®æŠ•ç¨¿æ•°: {len(recent_posts)}ä»¶")
    print("åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...\n")

    # å„æŠ•ç¨¿ã®åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    analytics_results = []
    for i, post in enumerate(recent_posts, 1):
        print(f"[{i}/{len(recent_posts)}] æŠ•ç¨¿ {post['csv_id']} ã‚’åˆ†æä¸­...")
        analytics = get_post_insights(post['threads_post_id'])
        if analytics:
            analytics['posted_at'] = datetime.fromisoformat(post['posted_at'])
            analytics['csv_id'] = post['csv_id']
            analytics['category'] = post['category']
            analytics_results.append(analytics)

            # DB ã«ä¿å­˜
            save_analytics(post['id'], analytics)
            print(f"  âœ“ å®Œäº†")
        time.sleep(2)

    if not analytics_results:
        print("âš ï¸  åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = generate_pdca_markdown(analytics_results, days)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(PDCA_REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nâœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ {PDCA_REPORT_FILE} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print("\n" + report)


def generate_pdca_markdown(analytics_results, days):
    """PDCAãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownå½¢å¼ã§ç”Ÿæˆ"""
    total_posts = len(analytics_results)
    total_views = sum(a['views'] for a in analytics_results)
    total_engagement = sum(a['engagement'] for a in analytics_results)
    avg_engagement_rate = sum(a['engagement_rate'] for a in analytics_results) / total_posts

    # æ™‚é–“å¸¯åˆ¥åˆ†æ
    hourly_performance = defaultdict(lambda: {'count': 0, 'engagement_rate': 0})
    for a in analytics_results:
        hour = a['posted_at'].hour
        hourly_performance[hour]['count'] += 1
        hourly_performance[hour]['engagement_rate'] += a['engagement_rate']

    for hour in hourly_performance:
        hourly_performance[hour]['avg_engagement_rate'] = (
            hourly_performance[hour]['engagement_rate'] /
            hourly_performance[hour]['count']
        )

    # ãƒ™ã‚¹ãƒˆæ™‚é–“å¸¯
    best_hours = sorted(
        hourly_performance.items(),
        key=lambda x: x[1]['avg_engagement_rate'],
        reverse=True
    )[:3]

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
    category_performance = defaultdict(lambda: {'count': 0, 'engagement_rate': 0})
    for a in analytics_results:
        cat = a.get('category', 'ãã®ä»–')
        category_performance[cat]['count'] += 1
        category_performance[cat]['engagement_rate'] += a['engagement_rate']

    for cat in category_performance:
        category_performance[cat]['avg_engagement_rate'] = (
            category_performance[cat]['engagement_rate'] /
            category_performance[cat]['count']
        )

    # ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿
    top_posts = sorted(analytics_results, key=lambda x: x['engagement_rate'], reverse=True)[:3]
    worst_posts = sorted(analytics_results, key=lambda x: x['engagement_rate'])[:3]

    # æ–‡å­—æ•°åˆ†æ
    char_counts = [len(a['text']) for a in analytics_results]
    avg_char_count = sum(char_counts) / len(char_counts) if char_counts else 0

    # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆJSTæ™‚åˆ»ã§è¡¨ç¤ºï¼‰
    jst = timezone(timedelta(hours=9))
    now = datetime.now(jst).strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')

    report = f"""# ğŸ“Š Threads PDCA ãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {now}
**åˆ†ææœŸé–“**: éå»{days}æ—¥é–“
**æŠ•ç¨¿æ•°**: {total_posts}ä»¶

---

## ğŸ“ˆ ã‚µãƒãƒªãƒ¼

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| **ç·è¡¨ç¤ºå›æ•°** | {total_views:,} |
| **ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ** | {total_engagement:,} |
| **å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡** | {avg_engagement_rate:.2f}% |
| **å¹³å‡æ–‡å­—æ•°** | {avg_char_count:.0f}æ–‡å­— |

---

## ğŸ† ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿

"""

    for i, post in enumerate(top_posts, 1):
        report += f"""### {i}ä½: {post['engagement_rate']:.2f}% [{post.get('category', '')}]

- **ãƒ†ã‚­ã‚¹ãƒˆ**: {post['text'][:80]}...
- **è¡¨ç¤ºå›æ•°**: {post['views']:,}
- **ã„ã„ã­**: {post['likes']:,} | **è¿”ä¿¡**: {post['replies']:,} | **ãƒªãƒã‚¹ãƒˆ**: {post['reposts']:,}
- **æŠ•ç¨¿æ™‚åˆ»**: {post['posted_at'].strftime('%m/%d %H:%M')}

"""

    report += f"""---

## ğŸ¯ ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

"""

    for cat, data in sorted(category_performance.items(), key=lambda x: x[1]['avg_engagement_rate'], reverse=True):
        report += f"- **{cat}**: {data['avg_engagement_rate']:.2f}% (æŠ•ç¨¿æ•°: {data['count']}ä»¶)\n"

    report += f"""

---

## â° ãƒ™ã‚¹ãƒˆæŠ•ç¨¿æ™‚é–“å¸¯

"""

    for i, (hour, data) in enumerate(best_hours, 1):
        report += f"{i}. **{hour:02d}æ™‚å°**: ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ {data['avg_engagement_rate']:.2f}% (æŠ•ç¨¿æ•°: {data['count']}ä»¶)\n"

    report += f"""

---

## ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ

### æ–‡å­—æ•°
- **å¹³å‡æ–‡å­—æ•°**: {avg_char_count:.0f}æ–‡å­—
- **æœ€çŸ­**: {min(char_counts)}æ–‡å­— | **æœ€é•·**: {max(char_counts)}æ–‡å­—

---

## âš ï¸ æ”¹å–„ãŒå¿…è¦ãªæŠ•ç¨¿

"""

    for i, post in enumerate(worst_posts, 1):
        report += f"""{i}. **{post['engagement_rate']:.2f}%** [{post.get('category', '')}] - {post['text'][:60]}...
"""

    report += f"""

---

## ğŸ’¡ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ (Plan)

### âœ… ç¶šã‘ã‚‹ã“ã¨ (Keep)

"""

    if best_hours:
        best_hour = best_hours[0][0]
        report += f"1. **{best_hour:02d}æ™‚å°ã®æŠ•ç¨¿ã‚’å¢—ã‚„ã™** - æœ€ã‚‚åå¿œãŒè‰¯ã„æ™‚é–“å¸¯ã§ã™\n"

    # ãƒˆãƒƒãƒ—ã‚«ãƒ†ã‚´ãƒª
    top_category = max(category_performance.items(), key=lambda x: x[1]['avg_engagement_rate'])
    report += f"2. **{top_category[0]}ã‚«ãƒ†ã‚´ãƒªãŒå¥½èª¿** - ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ {top_category[1]['avg_engagement_rate']:.2f}%\n"

    if top_posts:
        top_post = top_posts[0]
        if len(top_post['text']) < 200:
            report += f"3. **çŸ­ã‚ã®æŠ•ç¨¿ãŒå¥½èª¿** - {len(top_post['text'])}æ–‡å­—ç¨‹åº¦ãŒåŠ¹æœçš„\n"
        else:
            report += f"3. **é•·æ–‡æŠ•ç¨¿ãŒå¥½èª¿** - {len(top_post['text'])}æ–‡å­—ç¨‹åº¦ã®è©³ç´°ãªå†…å®¹ãŒåŠ¹æœçš„\n"

    report += f"""

### ğŸ”„ æ”¹å–„ã™ã‚‹ã“ã¨ (Improve)

"""

    # ãƒ¯ãƒ¼ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒª
    worst_category = min(category_performance.items(), key=lambda x: x[1]['avg_engagement_rate'])
    report += f"1. **{worst_category[0]}ã‚«ãƒ†ã‚´ãƒªã®æ”¹å–„** - ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç‡ãŒä½ã„ ({worst_category[1]['avg_engagement_rate']:.2f}%)\n"
    report += f"2. **ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŠ•ç¨¿ã®åˆ†æ** - ãªãœåå¿œãŒæ‚ªã‹ã£ãŸã®ã‹æŒ¯ã‚Šè¿”ã‚‹\n"

    report += f"""

### ğŸ†• è©¦ã™ã“ã¨ (Try)

1. **æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’è©¦ã™** - è³ªå•å½¢å¼ã€æŠ•ç¥¨ã€ãƒªã‚¹ãƒˆå½¢å¼ãªã©
2. **ç•°ãªã‚‹æ™‚é–“å¸¯ã«ãƒ†ã‚¹ãƒˆæŠ•ç¨¿** - ã¾ã è©¦ã—ã¦ã„ãªã„æ™‚é–“å¸¯ã‚’æ¢ã‚‹
3. **åå¿œãŒè‰¯ã„ã‚«ãƒ†ã‚´ãƒªã‚’å¢—ã‚„ã™** - {top_category[0]}ã®æŠ•ç¨¿ã‚’å¢—ã‚„ã™

---

**æ¬¡å›ãƒ¬ãƒãƒ¼ãƒˆ**: {days}æ—¥å¾Œ

"""

    return report


# ============================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================

def validate_config():
    """è¨­å®šã®æ¤œè¨¼"""
    global DRY_RUN

    print("\nâœ“ ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª")

    has_token = ACCESS_TOKEN != 'YOUR_ACCESS_TOKEN_HERE'
    has_user_id = USER_ID != 'YOUR_USER_ID_HERE'

    if not has_token:
        if DRY_RUN:
            print("  âš  THREADS_ACCESS_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        else:
            print("  âœ— THREADS_ACCESS_TOKEN ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
    else:
        masked_token = ACCESS_TOKEN[:8] + "..." if len(ACCESS_TOKEN) > 8 else "***"
        print(f"  âœ“ ACCESS_TOKEN: è¨­å®šæ¸ˆã¿ ({masked_token})")

    if not has_user_id:
        if DRY_RUN:
            print("  âš  THREADS_USER_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ˆãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        else:
            print("  âœ— THREADS_USER_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
    else:
        print(f"  âœ“ USER_ID: è¨­å®šæ¸ˆã¿ ({USER_ID})")

    print("\nâœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç¢ºèª")
    if not os.path.exists(DB_FILE):
        print(f"  âœ— {DB_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print(f"  â†’ python migrate_to_sqlite.py full ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return False
    else:
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM posts")
        count = cursor.fetchone()[0]
        conn.close()
        print(f"  âœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {DB_FILE}")
        print(f"  âœ“ æŠ•ç¨¿æ•°: {count}ä»¶")

    return True


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    global DRY_RUN, DB_FILE

    parser = argparse.ArgumentParser(
        description='Threads API äºˆç´„æŠ•ç¨¿ + PDCAåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ (SQLiteç‰ˆ)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # æŠ•ç¨¿å®Ÿè¡Œ
  python threads_sqlite.py post                # æŠ•ç¨¿ãƒã‚§ãƒƒã‚¯ï¼†å®Ÿè¡Œ
  python threads_sqlite.py post --dry-run      # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³

  # æŠ•ç¨¿ç®¡ç†
  python threads_sqlite.py list                # æŠ•ç¨¿ä¸€è¦§
  python threads_sqlite.py list --status pending --limit 10
  python threads_sqlite.py list --today        # ä»Šæ—¥ã®äºˆå®š

  python threads_sqlite.py add                 # æŠ•ç¨¿è¿½åŠ ï¼ˆå¯¾è©±å¼ï¼‰
  python threads_sqlite.py import --csv new.csv
  python threads_sqlite.py export --output backup.csv

  # PDCAåˆ†æ
  python threads_sqlite.py pdca                # éå»3æ—¥é–“
  python threads_sqlite.py pdca --days 7       # éå»7æ—¥é–“

  # å­¦ç¿’ãƒ­ã‚°æ›´æ–°
  python threads_sqlite.py update-learnings --text "ä»Šæ—¥ã®æ°—ã¥ã"
        """
    )

    parser.add_argument(
        'command',
        nargs='?',
        default='post',
        choices=['post', 'list', 'add', 'import', 'export', 'pdca', 'daily-report', 'update-learnings'],
        help='å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰'
    )

    parser.add_argument('--dry-run', action='store_true', help='ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--status', type=str, help='æŠ•ç¨¿ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ (pending/posted/failed)')
    parser.add_argument('--limit', type=int, default=20, help='è¡¨ç¤ºä»¶æ•°')
    parser.add_argument('--today', action='store_true', help='ä»Šæ—¥ã®æŠ•ç¨¿ã®ã¿')
    parser.add_argument('--tomorrow', action='store_true', help='æ˜æ—¥ã®æŠ•ç¨¿ã®ã¿')
    parser.add_argument('--csv', type=str, help='CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--output', type=str, help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--days', type=int, default=3, help='PDCAåˆ†æã®æ—¥æ•°')
    parser.add_argument('--datetime', type=str, help='æŠ•ç¨¿æ—¥æ™‚ (YYYY-MM-DD HH:MM)')
    parser.add_argument('--text', type=str, help='æŠ•ç¨¿ãƒ†ã‚­ã‚¹ãƒˆ')
    parser.add_argument('--category', type=str, help='ã‚«ãƒ†ã‚´ãƒª')

    args = parser.parse_args()

    if args.dry_run:
        DRY_RUN = True

    # ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
    if args.command == 'post':
        # æŠ•ç¨¿ãƒ¢ãƒ¼ãƒ‰
        print("=" * 70)
        if DRY_RUN:
            print("Threads äºˆç´„æŠ•ç¨¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰)")
        else:
            print("Threads äºˆç´„æŠ•ç¨¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
        print("=" * 70)

        if not validate_config():
            print("\nâš ï¸  è¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        migrate_db()

        check_and_post()

        if DRY_RUN:
            print("\n" + "=" * 70)
            print("ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Œäº†")
            print("å®Ÿéš›ã«æŠ•ç¨¿ã™ã‚‹å ´åˆã¯ --dry-run ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å¤–ã—ã¦ãã ã•ã„")
            print("=" * 70)

    elif args.command == 'list':
        list_posts(args.status, args.limit, args.today, args.tomorrow)

    elif args.command == 'add':
        if args.datetime and args.text:
            add_post(args.datetime, args.text, args.category)
        else:
            print("ä½¿ç”¨æ–¹æ³•: python threads_sqlite.py add --datetime \"2025-11-02 08:00\" --text \"æŠ•ç¨¿å†…å®¹\" --category \"æ‹æ„›\"")

    elif args.command == 'import':
        if args.csv:
            import_from_csv(args.csv)
        else:
            print("ä½¿ç”¨æ–¹æ³•: python threads_sqlite.py import --csv posts.csv")

    elif args.command == 'export':
        output = args.output or 'posts_export.csv'
        export_to_csv(output, args.status)

    elif args.command == 'pdca':
        generate_pdca_report(args.days)

    elif args.command == 'daily-report':
        create_daily_report()

    elif args.command == 'update-learnings':
        if not args.text:
            print("âœ— ã‚¨ãƒ©ãƒ¼: --text ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å­¦ç¿’å†…å®¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            print("ä¾‹: python threads_sqlite.py update-learnings --text 'çŸ­æ–‡ãƒ¡ã‚¿ç™ºè¨€ãŒåŠ¹æœçš„'")
            sys.exit(1)
        update_learnings(args.text)


if __name__ == '__main__':
    if '-h' in sys.argv or '--help' in sys.argv:
        main()
        sys.exit(0)

    is_dry_run = '--dry-run' in sys.argv
    is_read_only = any(cmd in sys.argv for cmd in ['list', 'export', 'pdca', 'add', 'import'])

    if not is_dry_run and not is_read_only:
        if ACCESS_TOKEN == 'YOUR_ACCESS_TOKEN_HERE' or USER_ID == 'YOUR_USER_ID_HERE':
            print("\nâš ï¸  èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼")
            print("ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„ï¼š")
            print("  export THREADS_ACCESS_TOKEN='your_token'")
            print("  export THREADS_USER_ID='your_user_id'")
            print("\nã¾ãŸã¯ã€å‹•ä½œç¢ºèªã ã‘ãªã‚‰ --dry-run ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼š")
            print("  python threads_sqlite.py post --dry-run")
            sys.exit(1)

    main()
